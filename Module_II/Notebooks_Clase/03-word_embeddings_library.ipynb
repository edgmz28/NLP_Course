{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo II: Vectores Palabra (Word Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T21:20:42.007937Z",
     "start_time": "2020-05-05T21:20:41.987638Z"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivación\n",
    "\n",
    "### El gran problema de Bag of Words\n",
    "\n",
    "Pensemos en estas 3 frases como documentos:\n",
    "\n",
    "- $doc_1$: `¡Buenísimo el croissant!`\n",
    "- $doc_2$: `¡Estuvo espectacular ese pan francés!`\n",
    "- $doc_3$: `!Buenísima esa pintura!`\n",
    "\n",
    "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo 🍞🍞👌 y que $doc_3$ 🎨 no tiene mucho que ver con los otros.\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "\n",
    "Para esto, generamos un modelo `Bag of Words` sobre el documento. Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento. \n",
    "\n",
    "Además, omitimos algunas stopwords y consideramos pan frances como un solo token.\n",
    "\n",
    "$$v = \\{buenísima, croissant, estuvo, espectacular, pan\\ francés, pintura\\}$$\n",
    "\n",
    "Entonces, el $\\vec{doc_1}$ quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "El $\\vec{doc_2}$ quedará:\n",
    "\n",
    "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} = \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el $\\vec{doc_3}$: \n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "**¿Cuál es el problema?**\n",
    "\n",
    "`buenísima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `croissant` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan francés` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buenísima` y `espectacular` son tan distintas como `croissant` y `pan francés`. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
    "\n",
    "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
    "\n",
    "$$d(doc_1, doc_2) = 2.236$$\n",
    "$$d(doc_1, doc_3) = 1.414$$\n",
    "\n",
    "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos están diciendo lo mismo!\n",
    "\n",
    "\n",
    "Nos gustaría que eso no sucediera. Que existiera algún método que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipótesis Distribucional\n",
    "\n",
    "Estamos buscando algún enfoque que nos permita representar las palabras de forma no aislada, si no como algo que además capture el significado de esta.\n",
    "\n",
    "Pensemos un poco en la **hipótesis distribucional**. Esta plantea que:\n",
    "\n",
    "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\" \n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "    \"Una palabra es caracterizada por la compañía que esta lleva.\"\n",
    "\n",
    "Esto nos puede hacer pensar que podríamos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Context Matrices\n",
    "\n",
    "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tamaño $k$ que rodean la palabra central. \n",
    "\n",
    "Cada fila representa a una palabra a través de su contexto. Como se puede ver, ya no es un vector one-hot, si no que ahora contiene mayor información.\n",
    "\n",
    "El tamaño de la matriz es el tamaño del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
    "\n",
    "<img src=\"./Figures/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Problema: Creada a partir de un corpus respetable, es gigantezca**. \n",
    "\n",
    "Por ejemplo, para $|v| = 100.000$, la matriz tendrá $\\frac{100000 * 100000 * 4}{10^9} = 40gb $.\n",
    "\n",
    "- Es caro mantenerla en memoria \n",
    "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldición de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
    "\n",
    "¿Habrá una mejor solución?\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.  Para esto, se usan distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
    "\n",
    "Volvamos a nuestro ejemplo anterior: `buenísima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... :\n",
    "\n",
    "`buenísima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `croissant`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cuál es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¿Cómo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "- Dependerá del modelo que utilizemos.\n",
    "\n",
    "\n",
    "##### Word2vec y Skip-gram\n",
    "\n",
    "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings. Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
    "\n",
    "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta consiste en que por cada palabra del dataset, predigamos las palabras de su contexto (las palabras presentes en ventana de algún tamaño $k$).\n",
    "\n",
    "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa serán los que usaremos como embeddings.\n",
    "\n",
    "#### Detalles del Modelo\n",
    "\n",
    "- Como dijimos, el modelo será una red de una sola capa. La capa oculta tendrá una dimensión $d$ la cual nosotros determinaremos. Esta capa no tendrá función de activación. Sin embargo, la de salida si, la cual será una softmax.\n",
    "\n",
    "- El vector de entrada, de tamaño $|V|$, será un vector one-hot de la palabra que estemos viendo en ese momento.\n",
    "\n",
    "- La salida, también de tamaño $|V|$, será un vector que contenga la distribución de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
    "\n",
    "- Al entrenar, se comparará la distribución de los contextos con la suma de los vectores one-hot del contexto real.\n",
    "\n",
    "<img src=\"./Figures/skip_gram_net_arch.png\" alt=\"Skip Gram\" style=\"width: 600px;\"/>\n",
    "\n",
    "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparición de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
    "\n",
    "La solución a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario.\n",
    "\n",
    "### La capa Oculta y los Embeddings\n",
    "\n",
    "Al terminar el entrenamiento, ¿Qué nos queda en la capa oculta?\n",
    "\n",
    "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representación continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representación continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "<img src=\"./Figures/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
    "\n",
    "¿Cómo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
    "\n",
    "<img src=\"./Figures/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementación de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install word2vec\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset y limpiar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "### Extracción de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:18.064454Z",
     "start_time": "2020-05-07T19:18:03.208281Z"
    }
   },
   "outputs": [],
   "source": [
    "#La condición para que sean considerados es que aparezcan por lo menos 100 veces repetidas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "### Definir el modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos parámetros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
    "- `window` : Tamaño de la ventana. Usaremos 4.\n",
    "- `size` : El tamaño de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero después de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
    "- `workers`: Cantidad de CPU que serán utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se creará un conjunto que contendrá (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuación, entenaremos el modelo. \n",
    "Los parámetros que usaremos serán: \n",
    "\n",
    "- `total_examples`: Número de documentos.\n",
    "- `epochs`: Número de veces que se iterará sobre el corpus.\n",
    "\n",
    "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:23:19.548567Z",
     "start_time": "2020-05-07T19:23:18.572531Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Palabras mas similares y Analogías\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la información contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matemático, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "      <th>publication_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yerko Roa</td>\n",
       "      <td>/lista/autores/yroa</td>\n",
       "      <td>Colapsa otro segmento de casa que se derrumbó ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>Noticia en Desarrollo  Estamos recopilando m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565778000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valentina González</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Policía busca a mujer acusada de matar a su pa...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Detectives de la Policía de Investigaciones ...</td>\n",
       "      <td>[#parricidio, #PDI, #Pudahuel, #Región Metropo...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Dos detenidos en Liceo de Aplicación: protagon...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Dos detenidos fue el saldo de una serie de i...</td>\n",
       "      <td>[#Incendio, #Liceo de Aplicación, #Región Metr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matías Vega</td>\n",
       "      <td>/lista/autores/mvega</td>\n",
       "      <td>Apoyo transversal: Senado aprueba en general p...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>La sala del Senado aprobó en general el proy...</td>\n",
       "      <td>[#Inmigración, #Inmigrantes, #Ley, #Migración,...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565772720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Valentina González</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Evacuación espontánea en Instituto Nacional po...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>La mañana de este miércoles se produjo una e...</td>\n",
       "      <td>[#Carabineros, #FFEE, #Gases Lacrimógenos, #In...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26408</th>\n",
       "      <td>Manuel Stuardo</td>\n",
       "      <td>/lista/autores/mstuardo</td>\n",
       "      <td>Naciones Unidas abre proceso de postulaciones ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>Las Naciones Unidas abrió un proceso de post...</td>\n",
       "      <td>[#cambio climático, #COP25, #Naciones Unidas, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565764200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26409</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Fernando Astengo chocó en estado de ebriedad e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>El exfutbolista Fernando Astengo protagonizó...</td>\n",
       "      <td>[#Accidente, #Fernando Astengo, #Peñalolén, #R...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565767440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26410</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Detuvieron a hombre que arrojó combustible a u...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Personal de Carabineros detuvo a un hombre q...</td>\n",
       "      <td>[#Indigente, #Parque Forestal, #Región Metropo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565769300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>Nicolás Parra</td>\n",
       "      <td>/lista/autores/nparra</td>\n",
       "      <td>Revelan identidad de 2 de 6 víctimas fatales e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>El intendente de Valparaíso, Jorge Martínez,...</td>\n",
       "      <td>[#derrumbe en valparaíso, #Región de Valparaís...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565771100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26412</th>\n",
       "      <td>Emilio Lara</td>\n",
       "      <td>/lista/autores/elara</td>\n",
       "      <td>Senado cerrará cuenta paralela al presupuesto ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>El presidente del Senado, Jaime Quintana (PP...</td>\n",
       "      <td>[#Dipres, #Fiscalía, #Hacienda, #Senado]</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26413 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author               author_link  \\\n",
       "0               Yerko Roa       /lista/autores/yroa   \n",
       "1      Valentina González  /lista/autores/vgonzalez   \n",
       "2          Felipe Delgado   /lista/autores/fdelgado   \n",
       "3             Matías Vega      /lista/autores/mvega   \n",
       "4      Valentina González  /lista/autores/vgonzalez   \n",
       "...                   ...                       ...   \n",
       "26408      Manuel Stuardo   /lista/autores/mstuardo   \n",
       "26409      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26410      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26411       Nicolás Parra     /lista/autores/nparra   \n",
       "26412         Emilio Lara      /lista/autores/elara   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Colapsa otro segmento de casa que se derrumbó ...   \n",
       "1      Policía busca a mujer acusada de matar a su pa...   \n",
       "2      Dos detenidos en Liceo de Aplicación: protagon...   \n",
       "3      Apoyo transversal: Senado aprueba en general p...   \n",
       "4      Evacuación espontánea en Instituto Nacional po...   \n",
       "...                                                  ...   \n",
       "26408  Naciones Unidas abre proceso de postulaciones ...   \n",
       "26409  Fernando Astengo chocó en estado de ebriedad e...   \n",
       "26410  Detuvieron a hombre que arrojó combustible a u...   \n",
       "26411  Revelan identidad de 2 de 6 víctimas fatales e...   \n",
       "26412  Senado cerrará cuenta paralela al presupuesto ...   \n",
       "\n",
       "                                                    link  category  \\\n",
       "0      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "1      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "2      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "3      https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "4      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "...                                                  ...       ...   \n",
       "26408  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "26409  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26410  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26411  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26412  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "\n",
       "                subcategory  \\\n",
       "0      region-de-valparaiso   \n",
       "1      region-metropolitana   \n",
       "2      region-metropolitana   \n",
       "3                     chile   \n",
       "4      region-metropolitana   \n",
       "...                     ...   \n",
       "26408                 chile   \n",
       "26409  region-metropolitana   \n",
       "26410  region-metropolitana   \n",
       "26411  region-de-valparaiso   \n",
       "26412                 chile   \n",
       "\n",
       "                                                 content  \\\n",
       "0        Noticia en Desarrollo  Estamos recopilando m...   \n",
       "1        Detectives de la Policía de Investigaciones ...   \n",
       "2        Dos detenidos fue el saldo de una serie de i...   \n",
       "3        La sala del Senado aprobó en general el proy...   \n",
       "4        La mañana de este miércoles se produjo una e...   \n",
       "...                                                  ...   \n",
       "26408    Las Naciones Unidas abrió un proceso de post...   \n",
       "26409    El exfutbolista Fernando Astengo protagonizó...   \n",
       "26410    Personal de Carabineros detuvo a un hombre q...   \n",
       "26411    El intendente de Valparaíso, Jorge Martínez,...   \n",
       "26412    El presidente del Senado, Jaime Quintana (PP...   \n",
       "\n",
       "                                                    tags  \\\n",
       "0                                                     []   \n",
       "1      [#parricidio, #PDI, #Pudahuel, #Región Metropo...   \n",
       "2      [#Incendio, #Liceo de Aplicación, #Región Metr...   \n",
       "3      [#Inmigración, #Inmigrantes, #Ley, #Migración,...   \n",
       "4      [#Carabineros, #FFEE, #Gases Lacrimógenos, #In...   \n",
       "...                                                  ...   \n",
       "26408  [#cambio climático, #COP25, #Naciones Unidas, ...   \n",
       "26409  [#Accidente, #Fernando Astengo, #Peñalolén, #R...   \n",
       "26410  [#Indigente, #Parque Forestal, #Región Metropo...   \n",
       "26411  [#derrumbe en valparaíso, #Región de Valparaís...   \n",
       "26412           [#Dipres, #Fiscalía, #Hacienda, #Senado]   \n",
       "\n",
       "                                          embedded_links  publication_datetime  \n",
       "0                                                     []         1565778000000  \n",
       "1      [https://media.biobiochile.cl/wp-content/uploa...         1565771820000  \n",
       "2                                                     []         1565772480000  \n",
       "3      [https://media.biobiochile.cl/wp-content/uploa...         1565772720000  \n",
       "4                                                     []         1565772960000  \n",
       "...                                                  ...                   ...  \n",
       "26408                                                 []         1565764200000  \n",
       "26409  [https://media.biobiochile.cl/wp-content/uploa...         1565767440000  \n",
       "26410                                                 []         1565769300000  \n",
       "26411                                                 []         1565771100000  \n",
       "26412  [https://media.biobiochile.cl/wp-content/uploa...         1565771640000  \n",
       "\n",
       "[26413 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json('Data/data')\n",
    "dataset_r = dataset.copy(deep=True)\n",
    "dataset_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset['title'] + dataset['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "stopwords = pd.read_csv(\"Data/spanish.txt\").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, lower = False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.transalate(str.maketrans('','', punctuation)).lower().split()\n",
    "    else:\n",
    "        tokenized_doc = doc.translate(str.maketrans('','', punctuation)).split\n",
    "    \n",
    "    tokenized_doc = [stemmer.stem(token) for token in tokenized_doc if token.lower() not in stopwords]\n",
    "    return tokenized_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-71310fb18a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleaned_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-71310fb18a8f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleaned_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-82fdd1991842>\u001b[0m in \u001b[0;36msimple_tokenizer\u001b[1;34m(doc, lower)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtokenized_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtokenized_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_doc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
