{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo II: Vectores Palabra (Word Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T21:20:42.007937Z",
     "start_time": "2020-05-05T21:20:41.987638Z"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivación\n",
    "\n",
    "### El gran problema de Bag of Words\n",
    "\n",
    "Pensemos en estas 3 frases como documentos:\n",
    "\n",
    "- $doc_1$: `¡Buenísimo el croissant!`\n",
    "- $doc_2$: `¡Estuvo espectacular ese pan francés!`\n",
    "- $doc_3$: `!Buenísima esa pintura!`\n",
    "\n",
    "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo 🍞🍞👌 y que $doc_3$ 🎨 no tiene mucho que ver con los otros.\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "\n",
    "Para esto, generamos un modelo `Bag of Words` sobre el documento. Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento. \n",
    "\n",
    "Además, omitimos algunas stopwords y consideramos pan frances como un solo token.\n",
    "\n",
    "$$v = \\{buenísima, croissant, estuvo, espectacular, pan\\ francés, pintura\\}$$\n",
    "\n",
    "Entonces, el $\\vec{doc_1}$ quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "El $\\vec{doc_2}$ quedará:\n",
    "\n",
    "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} = \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el $\\vec{doc_3}$: \n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "**¿Cuál es el problema?**\n",
    "\n",
    "`buenísima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `croissant` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan francés` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buenísima` y `espectacular` son tan distintas como `croissant` y `pan francés`. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
    "\n",
    "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
    "\n",
    "$$d(doc_1, doc_2) = 2.236$$\n",
    "$$d(doc_1, doc_3) = 1.414$$\n",
    "\n",
    "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos están diciendo lo mismo!\n",
    "\n",
    "\n",
    "Nos gustaría que eso no sucediera. Que existiera algún método que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipótesis Distribucional\n",
    "\n",
    "Estamos buscando algún enfoque que nos permita representar las palabras de forma no aislada, si no como algo que además capture el significado de esta.\n",
    "\n",
    "Pensemos un poco en la **hipótesis distribucional**. Esta plantea que:\n",
    "\n",
    "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\" \n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "    \"Una palabra es caracterizada por la compañía que esta lleva.\"\n",
    "\n",
    "Esto nos puede hacer pensar que podríamos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Context Matrices\n",
    "\n",
    "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tamaño $k$ que rodean la palabra central. \n",
    "\n",
    "Cada fila representa a una palabra a través de su contexto. Como se puede ver, ya no es un vector one-hot, si no que ahora contiene mayor información.\n",
    "\n",
    "El tamaño de la matriz es el tamaño del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
    "\n",
    "<img src=\"./Figures/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Problema: Creada a partir de un corpus respetable, es gigantezca**. \n",
    "\n",
    "Por ejemplo, para $|v| = 100.000$, la matriz tendrá $\\frac{100000 * 100000 * 4}{10^9} = 40gb $.\n",
    "\n",
    "- Es caro mantenerla en memoria \n",
    "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldición de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
    "\n",
    "¿Habrá una mejor solución?\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.  Para esto, se usan distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
    "\n",
    "Volvamos a nuestro ejemplo anterior: `buenísima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... :\n",
    "\n",
    "`buenísima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `croissant`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cuál es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¿Cómo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "- Dependerá del modelo que utilizemos.\n",
    "\n",
    "\n",
    "##### Word2vec y Skip-gram\n",
    "\n",
    "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings. Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
    "\n",
    "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta consiste en que por cada palabra del dataset, predigamos las palabras de su contexto (las palabras presentes en ventana de algún tamaño $k$).\n",
    "\n",
    "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa serán los que usaremos como embeddings.\n",
    "\n",
    "#### Detalles del Modelo\n",
    "\n",
    "- Como dijimos, el modelo será una red de una sola capa. La capa oculta tendrá una dimensión $d$ la cual nosotros determinaremos. Esta capa no tendrá función de activación. Sin embargo, la de salida si, la cual será una softmax.\n",
    "\n",
    "- El vector de entrada, de tamaño $|V|$, será un vector one-hot de la palabra que estemos viendo en ese momento.\n",
    "\n",
    "- La salida, también de tamaño $|V|$, será un vector que contenga la distribución de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
    "\n",
    "- Al entrenar, se comparará la distribución de los contextos con la suma de los vectores one-hot del contexto real.\n",
    "\n",
    "<img src=\"./Figures/skip_gram_net_arch.png\" alt=\"Skip Gram\" style=\"width: 600px;\"/>\n",
    "\n",
    "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparición de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
    "\n",
    "La solución a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario.\n",
    "\n",
    "### La capa Oculta y los Embeddings\n",
    "\n",
    "Al terminar el entrenamiento, ¿Qué nos queda en la capa oculta?\n",
    "\n",
    "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representación continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representación continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "<img src=\"./Figures/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
    "\n",
    "¿Cómo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
    "\n",
    "<img src=\"./Figures/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementación de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install word2vec\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset y limpiar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('Data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_r = dataset.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "      <th>publication_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yerko Roa</td>\n",
       "      <td>/lista/autores/yroa</td>\n",
       "      <td>Colapsa otro segmento de casa que se derrumbó ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>Noticia en Desarrollo  Estamos recopilando m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565778000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valentina González</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Policía busca a mujer acusada de matar a su pa...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Detectives de la Policía de Investigaciones ...</td>\n",
       "      <td>[#parricidio, #PDI, #Pudahuel, #Región Metropo...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Dos detenidos en Liceo de Aplicación: protagon...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Dos detenidos fue el saldo de una serie de i...</td>\n",
       "      <td>[#Incendio, #Liceo de Aplicación, #Región Metr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matías Vega</td>\n",
       "      <td>/lista/autores/mvega</td>\n",
       "      <td>Apoyo transversal: Senado aprueba en general p...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>La sala del Senado aprobó en general el proy...</td>\n",
       "      <td>[#Inmigración, #Inmigrantes, #Ley, #Migración,...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565772720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Valentina González</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Evacuación espontánea en Instituto Nacional po...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>La mañana de este miércoles se produjo una e...</td>\n",
       "      <td>[#Carabineros, #FFEE, #Gases Lacrimógenos, #In...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26408</th>\n",
       "      <td>Manuel Stuardo</td>\n",
       "      <td>/lista/autores/mstuardo</td>\n",
       "      <td>Naciones Unidas abre proceso de postulaciones ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>Las Naciones Unidas abrió un proceso de post...</td>\n",
       "      <td>[#cambio climático, #COP25, #Naciones Unidas, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565764200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26409</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Fernando Astengo chocó en estado de ebriedad e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>El exfutbolista Fernando Astengo protagonizó...</td>\n",
       "      <td>[#Accidente, #Fernando Astengo, #Peñalolén, #R...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565767440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26410</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Detuvieron a hombre que arrojó combustible a u...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Personal de Carabineros detuvo a un hombre q...</td>\n",
       "      <td>[#Indigente, #Parque Forestal, #Región Metropo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565769300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>Nicolás Parra</td>\n",
       "      <td>/lista/autores/nparra</td>\n",
       "      <td>Revelan identidad de 2 de 6 víctimas fatales e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>El intendente de Valparaíso, Jorge Martínez,...</td>\n",
       "      <td>[#derrumbe en valparaíso, #Región de Valparaís...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565771100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26412</th>\n",
       "      <td>Emilio Lara</td>\n",
       "      <td>/lista/autores/elara</td>\n",
       "      <td>Senado cerrará cuenta paralela al presupuesto ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>El presidente del Senado, Jaime Quintana (PP...</td>\n",
       "      <td>[#Dipres, #Fiscalía, #Hacienda, #Senado]</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26413 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author               author_link  \\\n",
       "0               Yerko Roa       /lista/autores/yroa   \n",
       "1      Valentina González  /lista/autores/vgonzalez   \n",
       "2          Felipe Delgado   /lista/autores/fdelgado   \n",
       "3             Matías Vega      /lista/autores/mvega   \n",
       "4      Valentina González  /lista/autores/vgonzalez   \n",
       "...                   ...                       ...   \n",
       "26408      Manuel Stuardo   /lista/autores/mstuardo   \n",
       "26409      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26410      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26411       Nicolás Parra     /lista/autores/nparra   \n",
       "26412         Emilio Lara      /lista/autores/elara   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Colapsa otro segmento de casa que se derrumbó ...   \n",
       "1      Policía busca a mujer acusada de matar a su pa...   \n",
       "2      Dos detenidos en Liceo de Aplicación: protagon...   \n",
       "3      Apoyo transversal: Senado aprueba en general p...   \n",
       "4      Evacuación espontánea en Instituto Nacional po...   \n",
       "...                                                  ...   \n",
       "26408  Naciones Unidas abre proceso de postulaciones ...   \n",
       "26409  Fernando Astengo chocó en estado de ebriedad e...   \n",
       "26410  Detuvieron a hombre que arrojó combustible a u...   \n",
       "26411  Revelan identidad de 2 de 6 víctimas fatales e...   \n",
       "26412  Senado cerrará cuenta paralela al presupuesto ...   \n",
       "\n",
       "                                                    link  category  \\\n",
       "0      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "1      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "2      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "3      https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "4      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "...                                                  ...       ...   \n",
       "26408  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "26409  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26410  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26411  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26412  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "\n",
       "                subcategory  \\\n",
       "0      region-de-valparaiso   \n",
       "1      region-metropolitana   \n",
       "2      region-metropolitana   \n",
       "3                     chile   \n",
       "4      region-metropolitana   \n",
       "...                     ...   \n",
       "26408                 chile   \n",
       "26409  region-metropolitana   \n",
       "26410  region-metropolitana   \n",
       "26411  region-de-valparaiso   \n",
       "26412                 chile   \n",
       "\n",
       "                                                 content  \\\n",
       "0        Noticia en Desarrollo  Estamos recopilando m...   \n",
       "1        Detectives de la Policía de Investigaciones ...   \n",
       "2        Dos detenidos fue el saldo de una serie de i...   \n",
       "3        La sala del Senado aprobó en general el proy...   \n",
       "4        La mañana de este miércoles se produjo una e...   \n",
       "...                                                  ...   \n",
       "26408    Las Naciones Unidas abrió un proceso de post...   \n",
       "26409    El exfutbolista Fernando Astengo protagonizó...   \n",
       "26410    Personal de Carabineros detuvo a un hombre q...   \n",
       "26411    El intendente de Valparaíso, Jorge Martínez,...   \n",
       "26412    El presidente del Senado, Jaime Quintana (PP...   \n",
       "\n",
       "                                                    tags  \\\n",
       "0                                                     []   \n",
       "1      [#parricidio, #PDI, #Pudahuel, #Región Metropo...   \n",
       "2      [#Incendio, #Liceo de Aplicación, #Región Metr...   \n",
       "3      [#Inmigración, #Inmigrantes, #Ley, #Migración,...   \n",
       "4      [#Carabineros, #FFEE, #Gases Lacrimógenos, #In...   \n",
       "...                                                  ...   \n",
       "26408  [#cambio climático, #COP25, #Naciones Unidas, ...   \n",
       "26409  [#Accidente, #Fernando Astengo, #Peñalolén, #R...   \n",
       "26410  [#Indigente, #Parque Forestal, #Región Metropo...   \n",
       "26411  [#derrumbe en valparaíso, #Región de Valparaís...   \n",
       "26412           [#Dipres, #Fiscalía, #Hacienda, #Senado]   \n",
       "\n",
       "                                          embedded_links  publication_datetime  \n",
       "0                                                     []         1565778000000  \n",
       "1      [https://media.biobiochile.cl/wp-content/uploa...         1565771820000  \n",
       "2                                                     []         1565772480000  \n",
       "3      [https://media.biobiochile.cl/wp-content/uploa...         1565772720000  \n",
       "4                                                     []         1565772960000  \n",
       "...                                                  ...                   ...  \n",
       "26408                                                 []         1565764200000  \n",
       "26409  [https://media.biobiochile.cl/wp-content/uploa...         1565767440000  \n",
       "26410                                                 []         1565769300000  \n",
       "26411                                                 []         1565771100000  \n",
       "26412  [https://media.biobiochile.cl/wp-content/uploa...         1565771640000  \n",
       "\n",
       "[26413 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Detectives de la Policía de Investigaciones realizan peritajes para detener a una mujer de 45 años, por su presunta responsabilidad en el ataque con un arma cortante contra su propio padre , lo que causó su muerte en la comuna de Pudahuel.  El hecho ocurrió en calle Presidente Truman, cerca de la intersección con Teniente Cruz, cuando, acorde a la declaración del hijo de la víctima y hermano de la victimaria, ambos sostuvieron un enfrentamiento verbal debido a la intensión de Hernan Silva Pérez de vender su casa .  Negocio que habría causado más que molestia en su hija, Tania Silva, quien tras la discusión habría acudido a la cocina de la vivienda para volver con un cuchillo y apuñalar a su padre.  Las primeras diligencias la realizaron carabineros de la 45º comisaría, quienes tomaron declaración al único testigo del crimen, al interior de la vivienda.  El capitán, Carlos Lagos, explicó que la principal hipótesis apunta a una discusión por el dinero de la venta de este inmueble.    Luego, fueron detectives de la Brigada de Homicidios de la PDI quienes realizaron peritajes, corroborando que la víctima murió al ser apuñalado en una ocasión a la altura del tórax. El subcomisario, Cristián Tur, aseguró que la presunta responsable está identificada.   Por esta razón es que hay diligencias de la policía enfocadas en dar con el paradero y detener a Tania Silva Herrera, de 45 años, que, acorde a lo expresado por sus familiares, viviría en situación de calle , pero que es buscada por el delito de parricidio.   Este artículo describe un proceso judicial en curso  Existe la posibilidad de que los cargos sean desestimados al finalizar la investigación, por lo cual NO se debe considerar al o los imputados como culpables hasta que la Justicia dicte sentencia en su contra. (Artículo 04 del Código Procesal Penal)   '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_r.iloc[1,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset['title'] + dataset['content'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26413,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Colapsa otro segmento de casa que se derrumbó en Valparaíso  Noticia en Desarrollo  Estamos recopilando más antecedentes sobre esta noticia, quédate atento a las actualizaciones.    Parte de la estructura restante de la casa que cayó ayer en Valparaíso colapsó en la mañana de este miércoles.  El hecho se produjo a las 10:15 horas, en la esquina de Aldunate con Huito, donde ayer murieron seis personas.  Según información preliminar, no había rescatistas en el lugar, porque las labores se habían suspendido por el peligro de seguir trabajando. No habría lesionados.  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "stopwords = pd.read_csv(\"Data/spanish.txt\").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'actualmente': 1,\n",
       "         'adelante': 1,\n",
       "         'además': 1,\n",
       "         'afirmó': 1,\n",
       "         'agregó': 1,\n",
       "         'ahora': 1,\n",
       "         'ahí': 1,\n",
       "         'al': 1,\n",
       "         'algo': 1,\n",
       "         'alguna': 1,\n",
       "         'algunas': 1,\n",
       "         'alguno': 1,\n",
       "         'algunos': 1,\n",
       "         'algún': 1,\n",
       "         'alrededor': 1,\n",
       "         'ambos': 1,\n",
       "         'ampleamos': 1,\n",
       "         'ante': 1,\n",
       "         'anterior': 1,\n",
       "         'antes': 1,\n",
       "         'apenas': 1,\n",
       "         'aproximadamente': 1,\n",
       "         'aquel': 1,\n",
       "         'aquellas': 1,\n",
       "         'aquellos': 1,\n",
       "         'aqui': 1,\n",
       "         'aquí': 1,\n",
       "         'arriba': 1,\n",
       "         'aseguró': 1,\n",
       "         'así': 1,\n",
       "         'atras': 1,\n",
       "         'aunque': 1,\n",
       "         'ayer': 1,\n",
       "         'añadió': 1,\n",
       "         'aún': 1,\n",
       "         'bajo': 1,\n",
       "         'bastante': 1,\n",
       "         'bien': 1,\n",
       "         'buen': 1,\n",
       "         'buena': 1,\n",
       "         'buenas': 1,\n",
       "         'bueno': 1,\n",
       "         'buenos': 1,\n",
       "         'cada': 1,\n",
       "         'casi': 1,\n",
       "         'cerca': 1,\n",
       "         'cierta': 1,\n",
       "         'ciertas': 1,\n",
       "         'cierto': 1,\n",
       "         'ciertos': 1,\n",
       "         'cinco': 1,\n",
       "         'comentó': 1,\n",
       "         'como': 1,\n",
       "         'con': 1,\n",
       "         'conocer': 1,\n",
       "         'conseguimos': 1,\n",
       "         'conseguir': 1,\n",
       "         'considera': 1,\n",
       "         'consideró': 1,\n",
       "         'consigo': 1,\n",
       "         'consigue': 1,\n",
       "         'consiguen': 1,\n",
       "         'consigues': 1,\n",
       "         'contra': 1,\n",
       "         'cosas': 1,\n",
       "         'creo': 1,\n",
       "         'cual': 1,\n",
       "         'cuales': 1,\n",
       "         'cualquier': 1,\n",
       "         'cuando': 1,\n",
       "         'cuanto': 1,\n",
       "         'cuatro': 1,\n",
       "         'cuenta': 1,\n",
       "         'cómo': 1,\n",
       "         'da': 1,\n",
       "         'dado': 1,\n",
       "         'dan': 1,\n",
       "         'dar': 1,\n",
       "         'de': 1,\n",
       "         'debe': 1,\n",
       "         'deben': 1,\n",
       "         'debido': 1,\n",
       "         'decir': 1,\n",
       "         'dejó': 1,\n",
       "         'del': 1,\n",
       "         'demás': 1,\n",
       "         'dentro': 1,\n",
       "         'desde': 1,\n",
       "         'después': 1,\n",
       "         'dice': 1,\n",
       "         'dicen': 1,\n",
       "         'dicho': 1,\n",
       "         'dieron': 1,\n",
       "         'diferente': 1,\n",
       "         'diferentes': 1,\n",
       "         'dijeron': 1,\n",
       "         'dijo': 1,\n",
       "         'dio': 1,\n",
       "         'donde': 1,\n",
       "         'dos': 1,\n",
       "         'durante': 1,\n",
       "         'e': 1,\n",
       "         'ejemplo': 1,\n",
       "         'el': 1,\n",
       "         'ella': 1,\n",
       "         'ellas': 1,\n",
       "         'ello': 1,\n",
       "         'ellos': 1,\n",
       "         'embargo': 1,\n",
       "         'empleais': 1,\n",
       "         'emplean': 1,\n",
       "         'emplear': 1,\n",
       "         'empleas': 1,\n",
       "         'empleo': 1,\n",
       "         'en': 1,\n",
       "         'encima': 1,\n",
       "         'encuentra': 1,\n",
       "         'entonces': 1,\n",
       "         'entre': 1,\n",
       "         'era': 1,\n",
       "         'erais': 1,\n",
       "         'eramos': 1,\n",
       "         'eran': 1,\n",
       "         'eras': 1,\n",
       "         'eres': 1,\n",
       "         'es': 1,\n",
       "         'esa': 1,\n",
       "         'esas': 1,\n",
       "         'ese': 1,\n",
       "         'eso': 1,\n",
       "         'esos': 1,\n",
       "         'esta': 1,\n",
       "         'estaba': 1,\n",
       "         'estabais': 1,\n",
       "         'estaban': 1,\n",
       "         'estabas': 1,\n",
       "         'estad': 1,\n",
       "         'estada': 1,\n",
       "         'estadas': 1,\n",
       "         'estado': 1,\n",
       "         'estados': 1,\n",
       "         'estais': 1,\n",
       "         'estamos': 1,\n",
       "         'estan': 1,\n",
       "         'estando': 1,\n",
       "         'estar': 1,\n",
       "         'estaremos': 1,\n",
       "         'estará': 1,\n",
       "         'estarán': 1,\n",
       "         'estarás': 1,\n",
       "         'estaré': 1,\n",
       "         'estaréis': 1,\n",
       "         'estaría': 1,\n",
       "         'estaríais': 1,\n",
       "         'estaríamos': 1,\n",
       "         'estarían': 1,\n",
       "         'estarías': 1,\n",
       "         'estas': 1,\n",
       "         'este': 1,\n",
       "         'estemos': 1,\n",
       "         'esto': 1,\n",
       "         'estos': 1,\n",
       "         'estoy': 1,\n",
       "         'estuve': 1,\n",
       "         'estuviera': 1,\n",
       "         'estuvierais': 1,\n",
       "         'estuvieran': 1,\n",
       "         'estuvieras': 1,\n",
       "         'estuvieron': 1,\n",
       "         'estuviese': 1,\n",
       "         'estuvieseis': 1,\n",
       "         'estuviesen': 1,\n",
       "         'estuvieses': 1,\n",
       "         'estuvimos': 1,\n",
       "         'estuviste': 1,\n",
       "         'estuvisteis': 1,\n",
       "         'estuviéramos': 1,\n",
       "         'estuviésemos': 1,\n",
       "         'estuvo': 1,\n",
       "         'está': 1,\n",
       "         'estábamos': 1,\n",
       "         'estáis': 1,\n",
       "         'están': 1,\n",
       "         'estás': 1,\n",
       "         'esté': 1,\n",
       "         'estéis': 1,\n",
       "         'estén': 1,\n",
       "         'estés': 1,\n",
       "         'ex': 1,\n",
       "         'existe': 1,\n",
       "         'existen': 1,\n",
       "         'explicó': 1,\n",
       "         'expresó': 1,\n",
       "         'fin': 1,\n",
       "         'fue': 1,\n",
       "         'fuera': 1,\n",
       "         'fuerais': 1,\n",
       "         'fueran': 1,\n",
       "         'fueras': 1,\n",
       "         'fueron': 1,\n",
       "         'fuese': 1,\n",
       "         'fueseis': 1,\n",
       "         'fuesen': 1,\n",
       "         'fueses': 1,\n",
       "         'fui': 1,\n",
       "         'fuimos': 1,\n",
       "         'fuiste': 1,\n",
       "         'fuisteis': 1,\n",
       "         'fuéramos': 1,\n",
       "         'fuésemos': 1,\n",
       "         'gran': 1,\n",
       "         'grandes': 1,\n",
       "         'gueno': 1,\n",
       "         'ha': 1,\n",
       "         'haber': 1,\n",
       "         'habida': 1,\n",
       "         'habidas': 1,\n",
       "         'habido': 1,\n",
       "         'habidos': 1,\n",
       "         'habiendo': 1,\n",
       "         'habremos': 1,\n",
       "         'habrá': 1,\n",
       "         'habrán': 1,\n",
       "         'habrás': 1,\n",
       "         'habré': 1,\n",
       "         'habréis': 1,\n",
       "         'habría': 1,\n",
       "         'habríais': 1,\n",
       "         'habríamos': 1,\n",
       "         'habrían': 1,\n",
       "         'habrías': 1,\n",
       "         'habéis': 1,\n",
       "         'había': 1,\n",
       "         'habíais': 1,\n",
       "         'habíamos': 1,\n",
       "         'habían': 1,\n",
       "         'habías': 1,\n",
       "         'hace': 1,\n",
       "         'haceis': 1,\n",
       "         'hacemos': 1,\n",
       "         'hacen': 1,\n",
       "         'hacer': 1,\n",
       "         'hacerlo': 1,\n",
       "         'haces': 1,\n",
       "         'hacia': 1,\n",
       "         'haciendo': 1,\n",
       "         'hago': 1,\n",
       "         'han': 1,\n",
       "         'has': 1,\n",
       "         'hasta': 1,\n",
       "         'hay': 1,\n",
       "         'haya': 1,\n",
       "         'hayamos': 1,\n",
       "         'hayan': 1,\n",
       "         'hayas': 1,\n",
       "         'hayáis': 1,\n",
       "         'he': 1,\n",
       "         'hecho': 1,\n",
       "         'hemos': 1,\n",
       "         'hicieron': 1,\n",
       "         'hizo': 1,\n",
       "         'hoy': 1,\n",
       "         'hube': 1,\n",
       "         'hubiera': 1,\n",
       "         'hubierais': 1,\n",
       "         'hubieran': 1,\n",
       "         'hubieras': 1,\n",
       "         'hubieron': 1,\n",
       "         'hubiese': 1,\n",
       "         'hubieseis': 1,\n",
       "         'hubiesen': 1,\n",
       "         'hubieses': 1,\n",
       "         'hubimos': 1,\n",
       "         'hubiste': 1,\n",
       "         'hubisteis': 1,\n",
       "         'hubiéramos': 1,\n",
       "         'hubiésemos': 1,\n",
       "         'hubo': 1,\n",
       "         'igual': 1,\n",
       "         'incluso': 1,\n",
       "         'indicó': 1,\n",
       "         'informó': 1,\n",
       "         'intenta': 1,\n",
       "         'intentais': 1,\n",
       "         'intentamos': 1,\n",
       "         'intentan': 1,\n",
       "         'intentar': 1,\n",
       "         'intentas': 1,\n",
       "         'intento': 1,\n",
       "         'ir': 1,\n",
       "         'junto': 1,\n",
       "         'la': 1,\n",
       "         'lado': 1,\n",
       "         'largo': 1,\n",
       "         'las': 1,\n",
       "         'le': 1,\n",
       "         'les': 1,\n",
       "         'llegó': 1,\n",
       "         'lleva': 1,\n",
       "         'llevar': 1,\n",
       "         'lo': 1,\n",
       "         'los': 1,\n",
       "         'luego': 1,\n",
       "         'lugar': 1,\n",
       "         'manera': 1,\n",
       "         'manifestó': 1,\n",
       "         'mayor': 1,\n",
       "         'me': 1,\n",
       "         'mediante': 1,\n",
       "         'mejor': 1,\n",
       "         'mencionó': 1,\n",
       "         'menos': 1,\n",
       "         'mi': 1,\n",
       "         'mientras': 1,\n",
       "         'mio': 1,\n",
       "         'mis': 1,\n",
       "         'misma': 1,\n",
       "         'mismas': 1,\n",
       "         'mismo': 1,\n",
       "         'mismos': 1,\n",
       "         'modo': 1,\n",
       "         'momento': 1,\n",
       "         'mucha': 1,\n",
       "         'muchas': 1,\n",
       "         'mucho': 1,\n",
       "         'muchos': 1,\n",
       "         'muy': 1,\n",
       "         'más': 1,\n",
       "         'mí': 1,\n",
       "         'mía': 1,\n",
       "         'mías': 1,\n",
       "         'mío': 1,\n",
       "         'míos': 1,\n",
       "         'nada': 1,\n",
       "         'nadie': 1,\n",
       "         'ni': 1,\n",
       "         'ninguna': 1,\n",
       "         'ningunas': 1,\n",
       "         'ninguno': 1,\n",
       "         'ningunos': 1,\n",
       "         'ningún': 1,\n",
       "         'no': 1,\n",
       "         'nos': 1,\n",
       "         'nosotras': 1,\n",
       "         'nosotros': 1,\n",
       "         'nuestra': 1,\n",
       "         'nuestras': 1,\n",
       "         'nuestro': 1,\n",
       "         'nuestros': 1,\n",
       "         'nueva': 1,\n",
       "         'nuevas': 1,\n",
       "         'nuevo': 1,\n",
       "         'nuevos': 1,\n",
       "         'nunca': 1,\n",
       "         'o': 1,\n",
       "         'ocho': 1,\n",
       "         'os': 1,\n",
       "         'otra': 1,\n",
       "         'otras': 1,\n",
       "         'otro': 1,\n",
       "         'otros': 1,\n",
       "         'para': 1,\n",
       "         'parece': 1,\n",
       "         'parte': 1,\n",
       "         'partir': 1,\n",
       "         'pasada': 1,\n",
       "         'pasado': 1,\n",
       "         'pero': 1,\n",
       "         'pesar': 1,\n",
       "         'poca': 1,\n",
       "         'pocas': 1,\n",
       "         'poco': 1,\n",
       "         'pocos': 1,\n",
       "         'podeis': 1,\n",
       "         'podemos': 1,\n",
       "         'poder': 1,\n",
       "         'podria': 1,\n",
       "         'podriais': 1,\n",
       "         'podriamos': 1,\n",
       "         'podrian': 1,\n",
       "         'podrias': 1,\n",
       "         'podrá': 1,\n",
       "         'podrán': 1,\n",
       "         'podría': 1,\n",
       "         'podrían': 1,\n",
       "         'poner': 1,\n",
       "         'por': 1,\n",
       "         'por qué': 1,\n",
       "         'porque': 1,\n",
       "         'posible': 1,\n",
       "         'primer': 1,\n",
       "         'primera': 1,\n",
       "         'primero': 1,\n",
       "         'primeros': 1,\n",
       "         'principalmente': 1,\n",
       "         'propia': 1,\n",
       "         'propias': 1,\n",
       "         'propio': 1,\n",
       "         'propios': 1,\n",
       "         'próximo': 1,\n",
       "         'próximos': 1,\n",
       "         'pudo': 1,\n",
       "         'pueda': 1,\n",
       "         'puede': 1,\n",
       "         'pueden': 1,\n",
       "         'puedo': 1,\n",
       "         'pues': 1,\n",
       "         'que': 1,\n",
       "         'quedó': 1,\n",
       "         'queremos': 1,\n",
       "         'quien': 1,\n",
       "         'quienes': 1,\n",
       "         'quiere': 1,\n",
       "         'quién': 1,\n",
       "         'qué': 1,\n",
       "         'realizado': 1,\n",
       "         'realizar': 1,\n",
       "         'realizó': 1,\n",
       "         'respecto': 1,\n",
       "         'sabe': 1,\n",
       "         'sabeis': 1,\n",
       "         'sabemos': 1,\n",
       "         'saben': 1,\n",
       "         'saber': 1,\n",
       "         'sabes': 1,\n",
       "         'se': 1,\n",
       "         'sea': 1,\n",
       "         'seamos': 1,\n",
       "         'sean': 1,\n",
       "         'seas': 1,\n",
       "         'segunda': 1,\n",
       "         'segundo': 1,\n",
       "         'según': 1,\n",
       "         'seis': 1,\n",
       "         'ser': 1,\n",
       "         'seremos': 1,\n",
       "         'será': 1,\n",
       "         'serán': 1,\n",
       "         'serás': 1,\n",
       "         'seré': 1,\n",
       "         'seréis': 1,\n",
       "         'sería': 1,\n",
       "         'seríais': 1,\n",
       "         'seríamos': 1,\n",
       "         'serían': 1,\n",
       "         'serías': 1,\n",
       "         'seáis': 1,\n",
       "         'señaló': 1,\n",
       "         'si': 1,\n",
       "         'sido': 1,\n",
       "         'siempre': 1,\n",
       "         'siendo': 1,\n",
       "         'siete': 1,\n",
       "         'sigue': 1,\n",
       "         'siguiente': 1,\n",
       "         'sin': 1,\n",
       "         'sino': 1,\n",
       "         'sobre': 1,\n",
       "         'sois': 1,\n",
       "         'sola': 1,\n",
       "         'solamente': 1,\n",
       "         'solas': 1,\n",
       "         'solo': 1,\n",
       "         'solos': 1,\n",
       "         'somos': 1,\n",
       "         'son': 1,\n",
       "         'soy': 1,\n",
       "         'su': 1,\n",
       "         'sus': 1,\n",
       "         'suya': 1,\n",
       "         'suyas': 1,\n",
       "         'suyo': 1,\n",
       "         'suyos': 1,\n",
       "         'sí': 1,\n",
       "         'sólo': 1,\n",
       "         'tal': 1,\n",
       "         'también': 1,\n",
       "         'tampoco': 1,\n",
       "         'tan': 1,\n",
       "         'tanto': 1,\n",
       "         'te': 1,\n",
       "         'tendremos': 1,\n",
       "         'tendrá': 1,\n",
       "         'tendrán': 1,\n",
       "         'tendrás': 1,\n",
       "         'tendré': 1,\n",
       "         'tendréis': 1,\n",
       "         'tendría': 1,\n",
       "         'tendríais': 1,\n",
       "         'tendríamos': 1,\n",
       "         'tendrían': 1,\n",
       "         'tendrías': 1,\n",
       "         'tened': 1,\n",
       "         'teneis': 1,\n",
       "         'tenemos': 1,\n",
       "         'tener': 1,\n",
       "         'tenga': 1,\n",
       "         'tengamos': 1,\n",
       "         'tengan': 1,\n",
       "         'tengas': 1,\n",
       "         'tengo': 1,\n",
       "         'tengáis': 1,\n",
       "         'tenida': 1,\n",
       "         'tenidas': 1,\n",
       "         'tenido': 1,\n",
       "         'tenidos': 1,\n",
       "         'teniendo': 1,\n",
       "         'tenéis': 1,\n",
       "         'tenía': 1,\n",
       "         'teníais': 1,\n",
       "         'teníamos': 1,\n",
       "         'tenían': 1,\n",
       "         'tenías': 1,\n",
       "         'tercera': 1,\n",
       "         'ti': 1,\n",
       "         'tiempo': 1,\n",
       "         'tiene': 1,\n",
       "         'tienen': 1,\n",
       "         'tienes': 1,\n",
       "         'toda': 1,\n",
       "         'todas': 1,\n",
       "         'todavía': 1,\n",
       "         'todo': 1,\n",
       "         'todos': 1,\n",
       "         'total': 1,\n",
       "         'trabaja': 1,\n",
       "         'trabajais': 1,\n",
       "         'trabajamos': 1,\n",
       "         'trabajan': 1,\n",
       "         'trabajar': 1,\n",
       "         'trabajas': 1,\n",
       "         'trabajo': 1,\n",
       "         'tras': 1,\n",
       "         'trata': 1,\n",
       "         'través': 1,\n",
       "         'tres': 1,\n",
       "         'tu': 1,\n",
       "         'tus': 1,\n",
       "         'tuve': 1,\n",
       "         'tuviera': 1,\n",
       "         'tuvierais': 1,\n",
       "         'tuvieran': 1,\n",
       "         'tuvieras': 1,\n",
       "         'tuvieron': 1,\n",
       "         'tuviese': 1,\n",
       "         'tuvieseis': 1,\n",
       "         'tuviesen': 1,\n",
       "         'tuvieses': 1,\n",
       "         'tuvimos': 1,\n",
       "         'tuviste': 1,\n",
       "         'tuvisteis': 1,\n",
       "         'tuviéramos': 1,\n",
       "         'tuviésemos': 1,\n",
       "         'tuvo': 1,\n",
       "         'tuya': 1,\n",
       "         'tuyas': 1,\n",
       "         'tuyo': 1,\n",
       "         'tuyos': 1,\n",
       "         'tú': 1,\n",
       "         'ultimo': 1,\n",
       "         'un': 1,\n",
       "         'una': 1,\n",
       "         'unas': 1,\n",
       "         'uno': 1,\n",
       "         'unos': 1,\n",
       "         'usa': 1,\n",
       "         'usais': 1,\n",
       "         'usamos': 1,\n",
       "         'usan': 1,\n",
       "         'usar': 1,\n",
       "         'usas': 1,\n",
       "         'uso': 1,\n",
       "         'usted': 1,\n",
       "         'va': 1,\n",
       "         'vais': 1,\n",
       "         'valor': 1,\n",
       "         'vamos': 1,\n",
       "         'van': 1,\n",
       "         'varias': 1,\n",
       "         'varios': 1,\n",
       "         'vaya': 1,\n",
       "         'veces': 1,\n",
       "         'ver': 1,\n",
       "         'verdad': 1,\n",
       "         'verdadera': 1,\n",
       "         'verdadero': 1,\n",
       "         'vez': 1,\n",
       "         'vosotras': 1,\n",
       "         'vosotros': 1,\n",
       "         'voy': 1,\n",
       "         'vuestra': 1,\n",
       "         'vuestras': 1,\n",
       "         'vuestro': 1,\n",
       "         'vuestros': 1,\n",
       "         'y': 1,\n",
       "         'ya': 1,\n",
       "         'yo': 1,\n",
       "         'él': 1,\n",
       "         'éramos': 1,\n",
       "         'ésta': 1,\n",
       "         'éstas': 1,\n",
       "         'éste': 1,\n",
       "         'éstos': 1,\n",
       "         'última': 1,\n",
       "         'últimas': 1,\n",
       "         'último': 1,\n",
       "         'últimos': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans('','',punctuation)).lower().split()\n",
    "    else:\n",
    "        tokenized_doc = doc.translate(str.maketrans('','',punctuation)).split()\n",
    "    \n",
    "    tokenized_doc = [stemmer.stem(token) for token in tokenized_doc if token.lower() not in stopwords]\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ministr',\n",
       " 'cubill',\n",
       " 'extension',\n",
       " 'par',\n",
       " 'docent',\n",
       " 'problem',\n",
       " 'propuest',\n",
       " 'ministr',\n",
       " 'educ',\n",
       " 'marcel',\n",
       " 'cubill',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'votacion',\n",
       " 'realiz',\n",
       " 'interior',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'decid',\n",
       " 'manten',\n",
       " 'par',\n",
       " 'pes',\n",
       " 'llam',\n",
       " 'president',\n",
       " 'gremi',\n",
       " 'mari',\n",
       " 'aguil',\n",
       " 'a',\n",
       " 'repleg',\n",
       " 'entrev',\n",
       " 'program',\n",
       " 'expres',\n",
       " 'bio',\n",
       " 'bio',\n",
       " 'radi',\n",
       " 'ministr',\n",
       " 'valor',\n",
       " 'dich',\n",
       " 'aguil',\n",
       " 'previ',\n",
       " 'a',\n",
       " 'votacion',\n",
       " 'asegur',\n",
       " 'efect',\n",
       " 'posit',\n",
       " 'vuelt',\n",
       " 'clas',\n",
       " 'par',\n",
       " 'profesor',\n",
       " 'extiend',\n",
       " 'seman',\n",
       " '255',\n",
       " 'vot',\n",
       " 'marc',\n",
       " 'diferent',\n",
       " 'sufragi',\n",
       " '95',\n",
       " 'colegi',\n",
       " 'funcion',\n",
       " 'normal',\n",
       " 'baj',\n",
       " '490',\n",
       " 'colegi',\n",
       " 'qued',\n",
       " 'par',\n",
       " 'a',\n",
       " 'ministeri',\n",
       " 'segu',\n",
       " 'negoci',\n",
       " 'ofrec',\n",
       " 'consider',\n",
       " 'mayor',\n",
       " 'vot',\n",
       " 'extend',\n",
       " 'moviliz',\n",
       " 'ministr',\n",
       " 'limit',\n",
       " 'a',\n",
       " 'esper',\n",
       " 'resuelv',\n",
       " 'dialog',\n",
       " 'intern',\n",
       " 'llev',\n",
       " 'a',\n",
       " 'cab',\n",
       " 'interior',\n",
       " 'gremi',\n",
       " 'line',\n",
       " 'sostuv',\n",
       " 'decision',\n",
       " 'president',\n",
       " 'colegi',\n",
       " 'punt',\n",
       " 'negoci',\n",
       " 'llam',\n",
       " 'a',\n",
       " 'acept',\n",
       " 'propuest',\n",
       " 'problem',\n",
       " 'propuest',\n",
       " 'esper',\n",
       " 'opinion',\n",
       " 'interior',\n",
       " 'colegi',\n",
       " 'decant',\n",
       " 'magisteri',\n",
       " 'pid',\n",
       " 'a',\n",
       " 'docent',\n",
       " 'ces',\n",
       " 'par',\n",
       " 'ofert',\n",
       " '45',\n",
       " 'mil',\n",
       " 'trimestral',\n",
       " 'a',\n",
       " 'educ',\n",
       " 'diferencial',\n",
       " 'consult',\n",
       " 'dond',\n",
       " 'problem',\n",
       " 'a',\n",
       " 'parec',\n",
       " 'jef',\n",
       " 'carter',\n",
       " 'reiter',\n",
       " 'dialog',\n",
       " 'intern',\n",
       " 'pendient',\n",
       " 'reconoc',\n",
       " 'pag',\n",
       " 'mencion',\n",
       " 'a',\n",
       " 'educ',\n",
       " 'diferencial',\n",
       " 'parvul',\n",
       " 'cubill',\n",
       " 'adelant',\n",
       " 'gobiern',\n",
       " 'dispuest',\n",
       " 'a',\n",
       " 'discut',\n",
       " 'nuev',\n",
       " 'punt',\n",
       " 'señal',\n",
       " 'qued',\n",
       " 'conform',\n",
       " 'plant',\n",
       " 'petitori',\n",
       " 'dia',\n",
       " 'pas',\n",
       " 'entramp',\n",
       " 'demor',\n",
       " 'empez',\n",
       " 'a',\n",
       " 'avanz',\n",
       " 'punt',\n",
       " 'acuerd',\n",
       " 'critic',\n",
       " 'cubill',\n",
       " 'llam',\n",
       " 'direct',\n",
       " 'a',\n",
       " 'colegi',\n",
       " '5',\n",
       " 'sig',\n",
       " 'par',\n",
       " 'a',\n",
       " 'ojal',\n",
       " 'depong',\n",
       " 'aguil',\n",
       " 'alcanc',\n",
       " 'par',\n",
       " 'docent',\n",
       " 'mostr',\n",
       " 'organiz',\n",
       " 'enfrent',\n",
       " 'a',\n",
       " 'gobi',\n",
       " 'empresarial',\n",
       " 'ministeri',\n",
       " 'energ',\n",
       " 'puest',\n",
       " 'plan',\n",
       " 'recuper',\n",
       " 'repar',\n",
       " 'dañ',\n",
       " 'produc',\n",
       " 'a',\n",
       " 'niñ',\n",
       " 'educ',\n",
       " 'public',\n",
       " 'paraliz',\n",
       " 'punt',\n",
       " 'exig',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'primer',\n",
       " 'seman',\n",
       " 'moviliz',\n",
       " 'ministr',\n",
       " 'particip',\n",
       " 'negoci',\n",
       " 'instanci',\n",
       " 'lider',\n",
       " 'subsecretari',\n",
       " 'line',\n",
       " 'descart',\n",
       " 'mea',\n",
       " 'culp',\n",
       " 'sum',\n",
       " 'a',\n",
       " 'mes',\n",
       " 'recien',\n",
       " 'cuart',\n",
       " 'seman',\n",
       " 'par',\n",
       " 'gobiern',\n",
       " 'actu',\n",
       " 'predisposicion',\n",
       " 'posicion',\n",
       " 'direct',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'habl',\n",
       " 'subsecrterari',\n",
       " 'gobiern',\n",
       " 'actu',\n",
       " 'opinion',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'convers',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'escuch',\n",
       " 'entrev',\n",
       " 'complet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_content[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "### Extracción de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:18.064454Z",
     "start_time": "2020-05-07T19:18:03.208281Z"
    }
   },
   "outputs": [],
   "source": [
    "#La condición para que sean considerados es que aparezcan por lo menos 100 veces repetidas.\n",
    "\n",
    "phrases = Phrases(cleaned_content, min_count = 100, progress_per = 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colaps': 584,\n",
       " 'segment': 175,\n",
       " 'colaps_segment': 1,\n",
       " 'cas': 17621,\n",
       " 'segment_cas': 1,\n",
       " 'derrumb': 536,\n",
       " 'cas_derrumb': 6,\n",
       " 'valparais': 2017,\n",
       " 'derrumb_valparais': 11,\n",
       " 'notici': 1675,\n",
       " 'valparais_notici': 1,\n",
       " 'desarroll': 4340,\n",
       " 'notici_desarroll': 8,\n",
       " 'recopil': 201,\n",
       " 'desarroll_recopil': 1,\n",
       " 'antecedent': 2320,\n",
       " 'recopil_antecedent': 25,\n",
       " 'antecedent_notici': 1,\n",
       " 'quedat': 12,\n",
       " 'notici_quedat': 1,\n",
       " 'atent': 1598,\n",
       " 'quedat_atent': 1,\n",
       " 'a': 241340,\n",
       " 'atent_a': 215,\n",
       " 'actualiz': 376,\n",
       " 'a_actualiz': 19,\n",
       " 'estructur': 624,\n",
       " 'actualiz_estructur': 1,\n",
       " 'restant': 200,\n",
       " 'estructur_restant': 1,\n",
       " 'restant_cas': 2,\n",
       " 'cay': 724,\n",
       " 'cas_cay': 2,\n",
       " 'cay_valparais': 1,\n",
       " 'valparais_colaps': 1,\n",
       " 'mañan': 2853,\n",
       " 'colaps_mañan': 2,\n",
       " 'miercol': 5322,\n",
       " 'mañan_miercol': 159,\n",
       " 'produj': 1014,\n",
       " 'miercol_produj': 3,\n",
       " 'produj_a': 70,\n",
       " '1015': 13,\n",
       " 'a_1015': 4,\n",
       " 'hor': 8655,\n",
       " '1015_hor': 3,\n",
       " 'esquin': 233,\n",
       " 'hor_esquin': 2,\n",
       " 'aldunat': 19,\n",
       " 'esquin_aldunat': 1,\n",
       " 'huit': 10,\n",
       " 'aldunat_huit': 8,\n",
       " 'mur': 3672,\n",
       " 'huit_mur': 1,\n",
       " 'person': 16681,\n",
       " 'mur_person': 44,\n",
       " 'inform': 10339,\n",
       " 'person_inform': 17,\n",
       " 'prelimin': 403,\n",
       " 'inform_prelimin': 167,\n",
       " 'rescat': 1968,\n",
       " 'prelimin_rescat': 1,\n",
       " 'labor': 1400,\n",
       " 'rescat_labor': 1,\n",
       " 'suspend': 1853,\n",
       " 'labor_suspend': 2,\n",
       " 'peligr': 1892,\n",
       " 'suspend_peligr': 1,\n",
       " 'segu': 3708,\n",
       " 'peligr_segu': 4,\n",
       " 'trabaj': 7554,\n",
       " 'segu_trabaj': 118,\n",
       " 'lesion': 2204,\n",
       " 'trabaj_lesion': 2,\n",
       " 'polic': 6804,\n",
       " 'busc': 6991,\n",
       " 'polic_busc': 33,\n",
       " 'busc_a': 355,\n",
       " 'muj': 5790,\n",
       " 'a_muj': 752,\n",
       " 'acus': 9550,\n",
       " 'muj_acus': 55,\n",
       " 'mat': 1770,\n",
       " 'acus_mat': 59,\n",
       " 'mat_a': 934,\n",
       " 'padr': 3706,\n",
       " 'a_padr': 498,\n",
       " 'discusion': 1129,\n",
       " 'padr_discusion': 2,\n",
       " 'vent': 1802,\n",
       " 'discusion_vent': 1,\n",
       " 'viviend': 2437,\n",
       " 'vent_viviend': 6,\n",
       " 'pudahuel': 140,\n",
       " 'viviend_pudahuel': 3,\n",
       " 'detectiv': 220,\n",
       " 'pudahuel_detectiv': 1,\n",
       " 'detectiv_polic': 19,\n",
       " 'investig': 12695,\n",
       " 'polic_investig': 654,\n",
       " 'realiz': 5745,\n",
       " 'investig_realiz': 82,\n",
       " 'peritaj': 302,\n",
       " 'realiz_peritaj': 52,\n",
       " 'deten': 5620,\n",
       " 'peritaj_deten': 1,\n",
       " 'deten_a': 353,\n",
       " '45': 787,\n",
       " 'muj_45': 11,\n",
       " 'años': 21771,\n",
       " '45_años': 148,\n",
       " 'presunt': 2345,\n",
       " 'años_presunt': 21,\n",
       " 'respons': 4713,\n",
       " 'presunt_respons': 136,\n",
       " 'ataqu': 4166,\n",
       " 'respons_ataqu': 68,\n",
       " 'arma': 1079,\n",
       " 'ataqu_arma': 20,\n",
       " 'cortant': 36,\n",
       " 'arma_cortant': 7,\n",
       " 'cortant_padr': 1,\n",
       " 'caus': 5307,\n",
       " 'padr_caus': 3,\n",
       " 'muert': 8445,\n",
       " 'caus_muert': 361,\n",
       " 'comun': 10804,\n",
       " 'muert_comun': 7,\n",
       " 'comun_pudahuel': 21,\n",
       " 'ocurr': 6019,\n",
       " 'pudahuel_ocurr': 4,\n",
       " 'call': 3640,\n",
       " 'ocurr_call': 32,\n",
       " 'president': 20387,\n",
       " 'call_president': 6,\n",
       " 'trum': 23,\n",
       " 'president_trum': 2,\n",
       " 'interseccion': 324,\n",
       " 'trum_interseccion': 2,\n",
       " 'tenient': 350,\n",
       " 'interseccion_tenient': 2,\n",
       " 'cruz': 1619,\n",
       " 'tenient_cruz': 5,\n",
       " 'acord': 1023,\n",
       " 'cruz_acord': 1,\n",
       " 'acord_a': 133,\n",
       " 'declar': 8661,\n",
       " 'a_declar': 416,\n",
       " 'hij': 5439,\n",
       " 'declar_hij': 14,\n",
       " 'victim': 5716,\n",
       " 'hij_victim': 30,\n",
       " 'herman': 1805,\n",
       " 'victim_herman': 8,\n",
       " 'victimari': 106,\n",
       " 'herman_victimari': 2,\n",
       " 'sostuv': 2507,\n",
       " 'victimari_sostuv': 1,\n",
       " 'enfrent': 3611,\n",
       " 'sostuv_enfrent': 3,\n",
       " 'verbal': 150,\n",
       " 'enfrent_verbal': 7,\n",
       " 'verbal_a': 18,\n",
       " 'intension': 10,\n",
       " 'a_intension': 1,\n",
       " 'hern': 46,\n",
       " 'intension_hern': 1,\n",
       " 'silv': 922,\n",
       " 'hern_silv': 1,\n",
       " 'perez': 920,\n",
       " 'silv_perez': 1,\n",
       " 'vend': 1380,\n",
       " 'perez_vend': 1,\n",
       " 'vend_cas': 8,\n",
       " 'negoci': 4755,\n",
       " 'cas_negoci': 7,\n",
       " 'negoci_caus': 1,\n",
       " 'molesti': 384,\n",
       " 'caus_molesti': 30,\n",
       " 'molesti_hij': 2,\n",
       " 'tani': 37,\n",
       " 'hij_tani': 1,\n",
       " 'tani_silv': 2,\n",
       " 'silv_discusion': 1,\n",
       " 'acud': 1130,\n",
       " 'discusion_acud': 2,\n",
       " 'acud_a': 539,\n",
       " 'cocin': 374,\n",
       " 'a_cocin': 27,\n",
       " 'cocin_viviend': 3,\n",
       " 'volv': 2700,\n",
       " 'viviend_volv': 1,\n",
       " 'cuchill': 358,\n",
       " 'volv_cuchill': 1,\n",
       " 'apuñal': 417,\n",
       " 'cuchill_apuñal': 3,\n",
       " 'apuñal_a': 119,\n",
       " 'primer': 696,\n",
       " 'padr_primer': 1,\n",
       " 'diligent': 766,\n",
       " 'primer_diligent': 33,\n",
       " 'diligent_realiz': 35,\n",
       " 'carabiner': 5079,\n",
       " 'realiz_carabiner': 11,\n",
       " '45º': 7,\n",
       " 'carabiner_45º': 1,\n",
       " 'comis': 697,\n",
       " '45º_comis': 1,\n",
       " 'tom': 6868,\n",
       " 'comis_tom': 1,\n",
       " 'tom_declar': 100,\n",
       " 'unic': 2866,\n",
       " 'declar_unic': 6,\n",
       " 'testig': 1280,\n",
       " 'unic_testig': 4,\n",
       " 'crim': 1229,\n",
       " 'testig_crim': 2,\n",
       " 'interior': 3664,\n",
       " 'crim_interior': 2,\n",
       " 'interior_viviend': 66,\n",
       " 'capitan': 679,\n",
       " 'viviend_capitan': 1,\n",
       " 'carl': 2272,\n",
       " 'capitan_carl': 12,\n",
       " 'lag': 1972,\n",
       " 'carl_lag': 3,\n",
       " 'principal': 3793,\n",
       " 'lag_principal': 2,\n",
       " 'hipotesis': 250,\n",
       " 'principal_hipotesis': 17,\n",
       " 'apunt': 2217,\n",
       " 'hipotesis_apunt': 6,\n",
       " 'apunt_a': 1007,\n",
       " 'a_discusion': 64,\n",
       " 'diner': 2661,\n",
       " 'discusion_diner': 1,\n",
       " 'diner_vent': 7,\n",
       " 'inmuebl': 485,\n",
       " 'vent_inmuebl': 6,\n",
       " 'inmuebl_detectiv': 1,\n",
       " 'brig': 868,\n",
       " 'detectiv_brig': 40,\n",
       " 'homicidi': 1712,\n",
       " 'brig_homicidi': 350,\n",
       " 'pdi': 1322,\n",
       " 'homicidi_pdi': 131,\n",
       " 'pdi_realiz': 16,\n",
       " 'corrobor': 171,\n",
       " 'peritaj_corrobor': 2,\n",
       " 'corrobor_victim': 2,\n",
       " 'victim_mur': 28,\n",
       " 'mur_apuñal': 8,\n",
       " 'ocasion': 1541,\n",
       " 'apuñal_ocasion': 10,\n",
       " 'ocasion_a': 90,\n",
       " 'altur': 975,\n",
       " 'a_altur': 505,\n",
       " 'torax': 64,\n",
       " 'altur_torax': 10,\n",
       " 'subcomisari': 114,\n",
       " 'torax_subcomisari': 1,\n",
       " 'cristian': 863,\n",
       " 'subcomisari_cristian': 3,\n",
       " 'tur': 11,\n",
       " 'cristian_tur': 2,\n",
       " 'tur_presunt': 1,\n",
       " 'identific': 2109,\n",
       " 'respons_identific': 11,\n",
       " 'razon': 2155,\n",
       " 'identific_razon': 4,\n",
       " 'razon_diligent': 1,\n",
       " 'diligent_polic': 8,\n",
       " 'enfoc': 371,\n",
       " 'polic_enfoc': 3,\n",
       " 'parader': 439,\n",
       " 'enfoc_parader': 1,\n",
       " 'parader_deten': 3,\n",
       " 'a_tani': 3,\n",
       " 'herrer': 143,\n",
       " 'silv_herrer': 1,\n",
       " 'herrer_45': 1,\n",
       " 'años_acord': 2,\n",
       " 'expres': 1849,\n",
       " 'a_expres': 42,\n",
       " 'familiar': 1376,\n",
       " 'expres_familiar': 1,\n",
       " 'viv': 4990,\n",
       " 'familiar_viv': 4,\n",
       " 'situacion': 7561,\n",
       " 'viv_situacion': 65,\n",
       " 'situacion_call': 211,\n",
       " 'call_busc': 3,\n",
       " 'delit': 3706,\n",
       " 'busc_delit': 3,\n",
       " 'parricidi': 71,\n",
       " 'delit_parricidi': 32,\n",
       " 'articul': 2502,\n",
       " 'parricidi_articul': 1,\n",
       " 'describ': 1152,\n",
       " 'articul_describ': 530,\n",
       " 'proces': 6189,\n",
       " 'describ_proces': 527,\n",
       " 'judicial': 2900,\n",
       " 'proces_judicial': 698,\n",
       " 'curs': 1583,\n",
       " 'judicial_curs': 536,\n",
       " 'posibil': 2456,\n",
       " 'curs_posibil': 526,\n",
       " 'carg': 6419,\n",
       " 'posibil_carg': 526,\n",
       " 'desestim': 826,\n",
       " 'carg_desestim': 527,\n",
       " 'finaliz': 1233,\n",
       " 'desestim_finaliz': 526,\n",
       " 'finaliz_investig': 535,\n",
       " 'consider': 5351,\n",
       " 'investig_consider': 542,\n",
       " 'imput': 2615,\n",
       " 'consider_imput': 530,\n",
       " 'culpabl': 1372,\n",
       " 'imput_culpabl': 528,\n",
       " 'justici': 5313,\n",
       " 'culpabl_justici': 529,\n",
       " 'dict': 1095,\n",
       " 'justici_dict': 529,\n",
       " 'sentenci': 1893,\n",
       " 'dict_sentenci': 568,\n",
       " 'sentenci_articul': 526,\n",
       " '04': 587,\n",
       " 'articul_04': 526,\n",
       " 'codig': 965,\n",
       " '04_codig': 526,\n",
       " 'procesal': 630,\n",
       " 'codig_procesal': 539,\n",
       " 'penal': 2203,\n",
       " 'procesal_penal': 556,\n",
       " 'lice': 493,\n",
       " 'deten_lice': 1,\n",
       " 'aplic': 2643,\n",
       " 'lice_aplic': 14,\n",
       " 'protagon': 427,\n",
       " 'aplic_protagon': 1,\n",
       " 'incendi': 2165,\n",
       " 'protagon_incendi': 1,\n",
       " 'bañ': 563,\n",
       " 'incendi_bañ': 1,\n",
       " 'quem': 1027,\n",
       " 'bañ_quem': 2,\n",
       " 'capuch': 27,\n",
       " 'quem_capuch': 1,\n",
       " 'overol': 28,\n",
       " 'capuch_overol': 1,\n",
       " 'overol_deten': 1,\n",
       " 'sald': 468,\n",
       " 'deten_sald': 4,\n",
       " 'seri': 2810,\n",
       " 'sald_seri': 1,\n",
       " 'incident': 1672,\n",
       " 'seri_incident': 26,\n",
       " 'incident_ocurr': 87,\n",
       " 'ocurr_mañan': 36,\n",
       " 'mañan_lice': 1,\n",
       " 'santiag': 3179,\n",
       " 'aplic_santiag': 2,\n",
       " 'inclu': 3828,\n",
       " 'santiag_inclu': 7,\n",
       " 'fogat': 36,\n",
       " 'inclu_fogat': 1,\n",
       " 'fogat_interior': 2,\n",
       " 'interior_bañ': 4,\n",
       " 'bañ_a': 14,\n",
       " '800': 361,\n",
       " 'a_800': 60,\n",
       " '800_mañan': 2,\n",
       " 'grup': 6321,\n",
       " 'mañan_grup': 10,\n",
       " 'encapuch': 478,\n",
       " 'grup_encapuch': 42,\n",
       " 'disturbi': 308,\n",
       " 'encapuch_disturbi': 1,\n",
       " 'disturbi_interseccion': 1,\n",
       " 'cumming': 13,\n",
       " 'interseccion_cumming': 2,\n",
       " 'erasm': 8,\n",
       " 'cumming_erasm': 1,\n",
       " 'escal': 886,\n",
       " 'erasm_escal': 1,\n",
       " 'escal_inclu': 3,\n",
       " 'barric': 221,\n",
       " 'inclu_barric': 2,\n",
       " 'evacu': 1075,\n",
       " 'barric_evacu': 1,\n",
       " 'espontane': 65,\n",
       " 'evacu_espontane': 3,\n",
       " 'institut': 1830,\n",
       " 'espontane_institut': 2,\n",
       " 'nacional': 10685,\n",
       " 'institut_nacional': 822,\n",
       " 'efect': 3858,\n",
       " 'nacional_efect': 7,\n",
       " 'bomb': 977,\n",
       " 'efect_bomb': 4,\n",
       " 'lacrimogen': 273,\n",
       " 'bomb_lacrimogen': 67,\n",
       " 'lacrimogen_carabiner': 7,\n",
       " 'carabiner_a': 135,\n",
       " 'lleg': 8504,\n",
       " 'a_lleg': 389,\n",
       " 'fuerz': 4530,\n",
       " 'lleg_fuerz': 8,\n",
       " 'especial': 4011,\n",
       " 'fuerz_especial': 280,\n",
       " 'especial_carabiner': 100,\n",
       " 'manifest': 4449,\n",
       " 'carabiner_manifest': 12,\n",
       " 'lanz': 2585,\n",
       " 'manifest_lanz': 27,\n",
       " 'lanz_bomb': 102,\n",
       " 'molotov': 222,\n",
       " 'bomb_molotov': 153,\n",
       " 'personal': 4093,\n",
       " 'molotov_personal': 3,\n",
       " 'huir': 354,\n",
       " 'personal_huir': 1,\n",
       " 'huir_interior': 1,\n",
       " 'establec': 4350,\n",
       " 'interior_establec': 44,\n",
       " 'report': 2143,\n",
       " 'establec_report': 1,\n",
       " 'gonzal': 462,\n",
       " 'report_gonzal': 1,\n",
       " 'urbin': 22,\n",
       " 'gonzal_urbin': 4,\n",
       " 'uniform': 968,\n",
       " 'urbin_uniform': 1,\n",
       " 'uniform_encapuch': 1,\n",
       " 'inici': 6343,\n",
       " 'encapuch_inici': 2,\n",
       " 'inici_fogat': 3,\n",
       " 'fogat_bañ': 1,\n",
       " 'quem_overol': 2,\n",
       " 'overol_capuch': 1,\n",
       " 'mochil': 240,\n",
       " 'capuch_mochil': 1,\n",
       " 'utiliz': 2866,\n",
       " 'mochil_utiliz': 2,\n",
       " 'utiliz_personal': 5,\n",
       " 'policial': 2457,\n",
       " 'personal_policial': 233,\n",
       " 'ingres': 4549,\n",
       " 'policial_ingres': 11,\n",
       " 'recint': 1850,\n",
       " 'ingres_recint': 42,\n",
       " 'logr': 4949,\n",
       " 'recint_logr': 7,\n",
       " 'logr_deten': 64,\n",
       " 'joven': 1567,\n",
       " 'a_joven': 242,\n",
       " 'port': 895,\n",
       " 'joven_port': 2,\n",
       " 'port_bomb': 11,\n",
       " 'incendiari': 282,\n",
       " 'bomb_incendiari': 15,\n",
       " 'incendiari_urbin': 1,\n",
       " 'autor': 8979,\n",
       " 'urbin_autor': 1,\n",
       " 'autor_lice': 1,\n",
       " 'indic': 4813,\n",
       " 'lice_indic': 1,\n",
       " 'indic_protagon': 2,\n",
       " 'hech': 3093,\n",
       " 'protagon_hech': 2,\n",
       " 'estudi': 6680,\n",
       " 'hech_estudi': 7,\n",
       " 'estudi_recint': 9,\n",
       " 'recint_investig': 5,\n",
       " 'investig_a': 395,\n",
       " 'colegi': 2260,\n",
       " 'a_colegi': 90,\n",
       " 'pertenec': 719,\n",
       " 'colegi_pertenec': 1,\n",
       " 'aprehend': 76,\n",
       " 'pertenec_aprehend': 1,\n",
       " 'alumn': 1127,\n",
       " 'aprehend_alumn': 2,\n",
       " 'alumn_colegi': 28,\n",
       " 'denunci': 6315,\n",
       " 'colegi_denunci': 1,\n",
       " 'indiscrimin': 53,\n",
       " 'denunci_indiscrimin': 2,\n",
       " 'gas': 1247,\n",
       " 'indiscrimin_gas': 1,\n",
       " 'gas_lacrimogen': 190,\n",
       " 'simil': 748,\n",
       " 'carabiner_simil': 1,\n",
       " 'simil_a': 194,\n",
       " 'a_ocurr': 253,\n",
       " 'ocurr_institut': 6,\n",
       " 'rodrig': 912,\n",
       " 'nacional_rodrig': 22,\n",
       " 'pin': 323,\n",
       " 'rodrig_pin': 94,\n",
       " 'rbb': 1103,\n",
       " 'pin_rbb': 94,\n",
       " 'bomber': 1839,\n",
       " 'rbb_bomber': 2,\n",
       " 'control': 4263,\n",
       " 'bomber_control': 24,\n",
       " 'control_situacion': 64,\n",
       " 'clas': 1566,\n",
       " 'situacion_clas': 4,\n",
       " 'clas_suspend': 27,\n",
       " 'apoy': 5496,\n",
       " 'transversal': 179,\n",
       " 'apoy_transversal': 18,\n",
       " 'sen': 1990,\n",
       " 'transversal_sen': 2,\n",
       " 'aprueb': 550,\n",
       " 'sen_aprueb': 33,\n",
       " 'general': 5267,\n",
       " 'aprueb_general': 12,\n",
       " 'proyect': 7306,\n",
       " 'general_proyect': 39,\n",
       " 'ley': 6092,\n",
       " 'proyect_ley': 898,\n",
       " 'migracion': 923,\n",
       " 'ley_migracion': 26,\n",
       " 'gobiern': 18603,\n",
       " 'migracion_gobiern': 6,\n",
       " 'sal': 7598,\n",
       " 'gobiern_sal': 21,\n",
       " 'sal_sen': 62,\n",
       " 'aprob': 3092,\n",
       " 'sen_aprob': 50,\n",
       " 'aprob_general': 45,\n",
       " 'present': 8889,\n",
       " 'migracion_present': 2,\n",
       " 'present_gobiern': 57,\n",
       " 'gobiern_inici': 33,\n",
       " 'entrar': 869,\n",
       " 'inici_entrar': 3,\n",
       " 'entrar_a': 196,\n",
       " 'fas': 374,\n",
       " 'a_fas': 39,\n",
       " 'fas_indic': 2,\n",
       " 'indic_discusion': 4,\n",
       " 'particul': 1287,\n",
       " 'discusion_particul': 21,\n",
       " 'particul_inici': 4,\n",
       " 'inici_busc': 83,\n",
       " 'modific': 629,\n",
       " 'busc_modific': 40,\n",
       " 'polit': 9539,\n",
       " 'modific_polit': 8,\n",
       " 'migratori': 1019,\n",
       " 'polit_migratori': 183,\n",
       " 'pais': 20632,\n",
       " 'migratori_pais': 19,\n",
       " 'reform': 2601,\n",
       " 'pais_reform': 13,\n",
       " 'año': 10074,\n",
       " 'reform_año': 6,\n",
       " '1975': 62,\n",
       " 'año_1975': 2,\n",
       " 'votacion': 1368,\n",
       " '1975_votacion': 1,\n",
       " 'resolv': 1117,\n",
       " 'votacion_resolv': 3,\n",
       " '41': 382,\n",
       " 'resolv_41': 1,\n",
       " 'vot': 4374,\n",
       " '41_vot': 7,\n",
       " 'vot_a': 564,\n",
       " 'favor': 2277,\n",
       " 'a_favor': 1294,\n",
       " '0': 187,\n",
       " 'favor_0': 2,\n",
       " '0_0': 24,\n",
       " 'abstencion': 255,\n",
       " '0_abstencion': 2,\n",
       " 'abrir': 677,\n",
       " 'abstencion_abrir': 1,\n",
       " 'period': 5677,\n",
       " 'abrir_period': 2,\n",
       " 'period_ingres': 5,\n",
       " 'ingres_indic': 24,\n",
       " 'intervencion': 1250,\n",
       " 'indic_intervencion': 1,\n",
       " 'senador': 2323,\n",
       " 'intervencion_senador': 5,\n",
       " 'part': 7815,\n",
       " 'senador_part': 48,\n",
       " 'social': 7569,\n",
       " 'part_social': 467,\n",
       " 'jos': 2059,\n",
       " 'social_jos': 16,\n",
       " 'miguel': 1053,\n",
       " 'jos_miguel': 121,\n",
       " 'insulz': 68,\n",
       " 'miguel_insulz': 46,\n",
       " 'insulz_apoy': 1,\n",
       " 'med': 6662,\n",
       " 'apoy_med': 20,\n",
       " 'med_tom': 158,\n",
       " 'tom_gobiern': 26,\n",
       " 'torn': 1230,\n",
       " 'gobiern_torn': 10,\n",
       " 'fluj': 467,\n",
       " 'torn_fluj': 1,\n",
       " 'fluj_migratori': 71,\n",
       " 'produc': 1651,\n",
       " 'migratori_produc': 3,\n",
       " 'crisis': 4356,\n",
       " 'produc_crisis': 3,\n",
       " 'crisis_polit': 315,\n",
       " 'polit_social': 120,\n",
       " 'venezuel': 7076,\n",
       " 'social_venezuel': 19,\n",
       " 'acuerd': 11685,\n",
       " 'venezuel_acuerd': 12,\n",
       " 'fij': 1076,\n",
       " 'acuerd_fij': 8,\n",
       " 'vis': 610,\n",
       " 'fij_vis': 1,\n",
       " 'vis_acuerd': 1,\n",
       " 'decision': 4331,\n",
       " 'acuerd_decision': 9,\n",
       " 'decision_tom': 223,\n",
       " 'consul': 635,\n",
       " 'tom_consul': 1,\n",
       " 'tacn': 86,\n",
       " 'consul_tacn': 10,\n",
       " 'lim': 949,\n",
       " 'tacn_lim': 2,\n",
       " 'lim_sostuv': 2,\n",
       " 'sostuv_senador': 2,\n",
       " 'renov': 972,\n",
       " 'senador_renov': 39,\n",
       " 'renov_nacional': 325,\n",
       " 'francisc': 1738,\n",
       " 'nacional_francisc': 27,\n",
       " 'chahuan': 92,\n",
       " 'francisc_chahuan': 66,\n",
       " 'calific': 2080,\n",
       " 'chahuan_calific': 1,\n",
       " 'legisl': 2941,\n",
       " 'calific_legisl': 2,\n",
       " 'modern': 330,\n",
       " 'legisl_modern': 5,\n",
       " 'asegur': 4650,\n",
       " 'modern_asegur': 1,\n",
       " 'equilibr': 93,\n",
       " 'asegur_equilibr': 1,\n",
       " 'derech': 7616,\n",
       " 'equilibr_derech': 2,\n",
       " 'migrant': 2558,\n",
       " 'derech_migrant': 18,\n",
       " 'migrant_pais': 26,\n",
       " 'regul': 1271,\n",
       " 'pais_regul': 9,\n",
       " 'regul_migracion': 3,\n",
       " 'migracion_inici': 2,\n",
       " 'continu': 4406,\n",
       " 'inici_continu': 4,\n",
       " 'tramit': 1335,\n",
       " 'continu_tramit': 14,\n",
       " 'form': 6958,\n",
       " 'tramit_form': 7,\n",
       " 'form_particul': 18,\n",
       " 'particul_sen': 1,\n",
       " 'carabiner_mañan': 5,\n",
       " 'produj_evacu': 1,\n",
       " 'espontane_comun': 1,\n",
       " 'educ': 3447,\n",
       " 'comun_educ': 55,\n",
       " 'educ_institut': 6,\n",
       " 'efect_gas': 2,\n",
       " 'carabiner_interior': 11,\n",
       " 'accion': 4088,\n",
       " 'establec_accion': 8,\n",
       " 'accion_policial': 26,\n",
       " 'respuest': 2347,\n",
       " 'policial_respuest': 1,\n",
       " 'respuest_a': 481,\n",
       " 'a_accion': 201,\n",
       " 'accion_grup': 16,\n",
       " 'menor': 4738,\n",
       " 'grup_menor': 10,\n",
       " 'menor_encapuch': 2,\n",
       " 'encapuch_enfrent': 10,\n",
       " 'enfrent_carabiner': 19,\n",
       " 'techumbr': 74,\n",
       " 'carabiner_techumbr': 1,\n",
       " 'profesor': 2443,\n",
       " 'techumbr_profesor': 1,\n",
       " 'claudi': 704,\n",
       " 'profesor_claudi': 2,\n",
       " 'segovi': 29,\n",
       " 'claudi_segovi': 3,\n",
       " 'segovi_sal': 1,\n",
       " 'sal_recint': 17,\n",
       " 'recint_call': 3,\n",
       " 'alons': 88,\n",
       " 'call_alons': 2,\n",
       " 'ovall': 128,\n",
       " 'alons_ovall': 9,\n",
       " 'ovall_espontane': 1,\n",
       " 'orden': 4527,\n",
       " 'espontane_orden': 2,\n",
       " 'rector': 324,\n",
       " 'orden_rector': 1,\n",
       " 'vieron': 459,\n",
       " 'rector_vieron': 1,\n",
       " 'afect': 6231,\n",
       " 'vieron_afect': 93,\n",
       " 'afect_gas': 8,\n",
       " 'gas_policial': 1,\n",
       " 'policial_acus': 7,\n",
       " 'actu': 1592,\n",
       " 'acus_actu': 22,\n",
       " 'protocol': 788,\n",
       " 'actu_protocol': 5,\n",
       " 'protocol_fuerz': 3,\n",
       " 'advirt': 2010,\n",
       " 'especial_advirt': 1,\n",
       " 'trat': 4361,\n",
       " 'advirt_trat': 6,\n",
       " 'trat_situacion': 20,\n",
       " 'amerit': 81,\n",
       " 'situacion_amerit': 7,\n",
       " 'sumari': 694,\n",
       " 'amerit_sumari': 1,\n",
       " 'intern': 2230,\n",
       " 'sumari_intern': 40,\n",
       " 'numer': 3346,\n",
       " 'intern_numer': 2,\n",
       " 'numer_estudi': 15,\n",
       " 'encontr': 5960,\n",
       " 'estudi_encontr': 18,\n",
       " 'encontr_clas': 7,\n",
       " 'clas_vieron': 1,\n",
       " 'afect_situacion': 33,\n",
       " 'situacion_produj': 13,\n",
       " 'produj_estudi': 2,\n",
       " 'encar': 79,\n",
       " 'estudi_encar': 1,\n",
       " 'encar_a': 19,\n",
       " 'a_carabiner': 474,\n",
       " 'a_sal': 585,\n",
       " 'sal_establec': 13,\n",
       " 'establec_situacion': 10,\n",
       " 'deriv': 1168,\n",
       " 'situacion_deriv': 10,\n",
       " 'empujon': 33,\n",
       " 'deriv_empujon': 1,\n",
       " 'empujon_grup': 1,\n",
       " 'product': 4469,\n",
       " 'grup_product': 4,\n",
       " 'product_person': 6,\n",
       " 'result': 5882,\n",
       " 'person_result': 265,\n",
       " 'result_deten': 14,\n",
       " 'alcald': 2708,\n",
       " 'sharp': 135,\n",
       " 'alcald_sharp': 15,\n",
       " 'lament': 1797,\n",
       " 'sharp_lament': 1,\n",
       " 'mortal': 439,\n",
       " 'lament_mortal': 1,\n",
       " 'mortal_derrumb': 1,\n",
       " 'afirm': 2409,\n",
       " 'derrumb_afirm': 1,\n",
       " 'afirm_valparais': 2,\n",
       " 'ciud': 5032,\n",
       " 'valparais_ciud': 9,\n",
       " 'ciud_viv': 8,\n",
       " 'riesg': 2728,\n",
       " 'viv_riesg': 1,\n",
       " 'riesg_alcald': 1,\n",
       " 'alcald_valparais': 29,\n",
       " 'jorg': 1647,\n",
       " 'valparais_jorg': 52,\n",
       " 'jorg_sharp': 74,\n",
       " 'refir': 1250,\n",
       " 'sharp_refir': 4,\n",
       " 'refir_mañan': 5,\n",
       " 'miercol_mortal': 1,\n",
       " 'derrumbr': 1,\n",
       " 'mortal_derrumbr': 1,\n",
       " 'registr': 6542,\n",
       " 'derrumbr_registr': 1,\n",
       " 'anoch': 97,\n",
       " 'registr_anoch': 1,\n",
       " 'anoch_aldunat': 1,\n",
       " 'cerr': 2943,\n",
       " 'huit_cerr': 1,\n",
       " 'bellav': 53,\n",
       " 'cerr_bellav': 1,\n",
       " 'extend': 1115,\n",
       " 'bellav_extend': 1,\n",
       " 'condolent': 154,\n",
       " 'extend_condolent': 3,\n",
       " 'condolent_familiar': 1,\n",
       " 'familiar_victim': 106,\n",
       " 'victim_asegur': 8,\n",
       " 'asegur_ciud': 2,\n",
       " 'conmocion': 267,\n",
       " 'ciud_conmocion': 1,\n",
       " 'conmocion_ocurr': 1,\n",
       " 'ocurr_derrumb': 6,\n",
       " 'derrumb_viviend': 26,\n",
       " 'viviend_cerr': 14,\n",
       " 'cerr_valparais': 25,\n",
       " 'valparais_rescat': 7,\n",
       " '2': 2612,\n",
       " 'rescat_2': 8,\n",
       " 'cuerp': 3012,\n",
       " '2_cuerp': 9,\n",
       " '4': 2661,\n",
       " 'cuerp_4': 7,\n",
       " '4_victim': 9,\n",
       " 'fatal': 589,\n",
       " 'victim_fatal': 230,\n",
       " 'fatal_valparais': 1,\n",
       " 'usted': 374,\n",
       " 'valparais_usted': 1,\n",
       " 'dolor': 973,\n",
       " 'usted_dolor': 1,\n",
       " 'dia': 5959,\n",
       " 'dolor_dia': 1,\n",
       " 'compart': 1868,\n",
       " 'dia_compart': 1,\n",
       " 'compart_ciud': 3,\n",
       " 'ciud_lament': 2,\n",
       " 'lament_valparais': 1,\n",
       " 'permanent': 946,\n",
       " 'viv_permanent': 3,\n",
       " 'permanent_riesg': 4,\n",
       " 'riesg_ciud': 2,\n",
       " 'expuest': 415,\n",
       " 'ciud_expuest': 1,\n",
       " 'expuest_a': 133,\n",
       " 'a_situacion': 438,\n",
       " 'usual': 100,\n",
       " 'situacion_usual': 1,\n",
       " 'gust': 947,\n",
       " 'usual_gust': 1,\n",
       " 'sent': 3346,\n",
       " 'gust_sent': 1,\n",
       " 'profundiz': 279,\n",
       " 'sent_profundiz': 7,\n",
       " 'tip': 3493,\n",
       " 'profundiz_tip': 1,\n",
       " 'desliz': 240,\n",
       " 'tip_desliz': 1,\n",
       " 'evident': 1196,\n",
       " 'desliz_evident': 1,\n",
       " 'magnitud': 649,\n",
       " 'evident_magnitud': 7,\n",
       " 'magnitud_produc': 2,\n",
       " 'produc_ciud': 2,\n",
       " 'ciud_alcald': 6,\n",
       " 'esper': 8092,\n",
       " 'alcald_esper': 3,\n",
       " 'esper_trabaj': 9,\n",
       " 'pued': 1782,\n",
       " 'trabaj_pued': 6,\n",
       " 'proporcion': 540,\n",
       " 'pued_proporcion': 1,\n",
       " 'mayor': 4884,\n",
       " 'proporcion_mayor': 5,\n",
       " 'certez': 248,\n",
       " 'mayor_certez': 3,\n",
       " 'asimim': 1,\n",
       " 'certez_asimim': 1,\n",
       " 'asimim_viviend': 1,\n",
       " 'catastr': 108,\n",
       " 'viviend_catastr': 1,\n",
       " 'catastr_advirt': 1,\n",
       " 'prefier': 258,\n",
       " 'advirt_prefier': 1,\n",
       " 'prudent': 99,\n",
       " 'prefier_prudent': 1,\n",
       " 'prudent_investig': 1,\n",
       " 'investig_curs': 56,\n",
       " 'fiscal': 8908,\n",
       " 'curs_fiscal': 3,\n",
       " 'fiscal_a': 206,\n",
       " 'colabor': 1228,\n",
       " 'a_colabor': 102,\n",
       " 'esclarec': 351,\n",
       " 'colabor_esclarec': 3,\n",
       " 'esclarec_razon': 2,\n",
       " 'jov': 4250,\n",
       " 'jov_result': 27,\n",
       " 'grav': 2643,\n",
       " 'result_grav': 13,\n",
       " 'grav_apuñal': 4,\n",
       " 'event': 1298,\n",
       " 'apuñal_event': 1,\n",
       " 'benef': 30,\n",
       " 'event_benef': 5,\n",
       " 'escuel': 1491,\n",
       " 'benef_escuel': 2,\n",
       " 'chonchi': 43,\n",
       " 'escuel_chonchi': 1,\n",
       " 'chonchi_jov': 1,\n",
       " 'jov_grav': 2,\n",
       " 'medi': 11323,\n",
       " 'apuñal_medi': 1,\n",
       " 'fiest': 721,\n",
       " 'medi_fiest': 7,\n",
       " 'fiest_chonchi': 1,\n",
       " 'provinci': 1980,\n",
       " 'chonchi_provinci': 3,\n",
       " 'chilo': 290,\n",
       " 'provinci_chilo': 41,\n",
       " 'chilo_intern': 1,\n",
       " 'hospital': 3518,\n",
       " 'intern_hospital': 107,\n",
       " 'august': 388,\n",
       " 'hospital_august': 6,\n",
       " 'riffart': 6,\n",
       " 'august_riffart': 6,\n",
       " 'castr': 696,\n",
       " 'riffart_castr': 4,\n",
       " 'permanec': 1363,\n",
       " 'castr_permanec': 2,\n",
       " 'permanec_victim': 3,\n",
       " 'agresion': 1060,\n",
       " 'victim_agresion': 35,\n",
       " 'sufr': 2913,\n",
       " 'agresion_sufr': 32,\n",
       " 'sufr_arma': 1,\n",
       " 'cortopunz': 112,\n",
       " 'arma_cortopunz': 41,\n",
       " 'sector': 5870,\n",
       " 'cortopunz_sector': 2,\n",
       " 'petan': 1,\n",
       " 'sector_petan': 1,\n",
       " 'petan_produj': 1,\n",
       " 'produj_medi': 11,\n",
       " 'medi_event': 4,\n",
       " 'escuel_miguel': 3,\n",
       " 'aguil': 423,\n",
       " 'miguel_aguil': 8,\n",
       " 'aguil_carabiner': 1,\n",
       " 'detall': 4957,\n",
       " 'carabiner_detall': 19,\n",
       " 'agresor': 362,\n",
       " 'detall_agresor': 3,\n",
       " '20': 3264,\n",
       " 'agresor_20': 1,\n",
       " '20_años': 682,\n",
       " 'años_deten': 178,\n",
       " 'funcionari': 3785,\n",
       " 'deten_funcionari': 13,\n",
       " 'funcionari_policial': 105,\n",
       " 'detencion': 2410,\n",
       " 'policial_detencion': 11,\n",
       " 'detencion_agresor': 4,\n",
       " 'ampli': 1904,\n",
       " 'agresor_ampli': 1,\n",
       " 'juev': 5457,\n",
       " 'ampli_juev': 8,\n",
       " 'juev_a': 378,\n",
       " 'a_fiscal': 541,\n",
       " 'fiscal_esper': 18,\n",
       " 'esper_inform': 59,\n",
       " 'medic': 3730,\n",
       " 'inform_medic': 28,\n",
       " 'medic_lesion': 5,\n",
       " 'lesion_present': 10,\n",
       " 'present_victim': 12,\n",
       " 'prision': 2950,\n",
       " 'prevent': 1795,\n",
       " 'prision_prevent': 1174,\n",
       " 'qued': 4620,\n",
       " 'prevent_qued': 51,\n",
       " 'qued_acus': 15,\n",
       " 'viol': 1160,\n",
       " 'acus_viol': 87,\n",
       " 'dej': 6638,\n",
       " 'viol_dej': 2,\n",
       " 'embaraz': 779,\n",
       " 'dej_embaraz': 1,\n",
       " 'embaraz_a': 52,\n",
       " 'a_herman': 279,\n",
       " 'deficient': 200,\n",
       " 'herman_deficient': 1,\n",
       " 'mental': 444,\n",
       " 'deficient_mental': 2,\n",
       " 'mental_cas': 2,\n",
       " 'criminal': 1093,\n",
       " 'cas_criminal': 6,\n",
       " 'criminal_investig': 4,\n",
       " 'desformaliz': 17,\n",
       " 'investig_desformaliz': 14,\n",
       " 'desformaliz_año': 1,\n",
       " 'año_pdi': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ministr',\n",
       " 'viviend',\n",
       " 'enfrent',\n",
       " 'polem',\n",
       " 'casit',\n",
       " 'call',\n",
       " 'oficin',\n",
       " 'ministr',\n",
       " 'viviend_urban',\n",
       " 'cristi',\n",
       " 'monckeberg',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'polem',\n",
       " 'reflexion',\n",
       " 'integr',\n",
       " 'social',\n",
       " 'deficit',\n",
       " 'habitacional',\n",
       " 'pais',\n",
       " 'comision',\n",
       " 'viviend_urban',\n",
       " 'sen',\n",
       " 'mayor',\n",
       " 'propietari',\n",
       " 'patrimoni',\n",
       " 'casit',\n",
       " 'departament',\n",
       " 'radic',\n",
       " 'patrimoni',\n",
       " 'chilen',\n",
       " 'episodi',\n",
       " 'entrev',\n",
       " 'program',\n",
       " 'peor',\n",
       " 'juli',\n",
       " 'ces',\n",
       " 'rodriguez',\n",
       " 'remarc',\n",
       " 'dich',\n",
       " 'intervencion',\n",
       " 'larg',\n",
       " '14',\n",
       " 'agost',\n",
       " 'siqu',\n",
       " 'sac',\n",
       " 'context',\n",
       " 'escuch',\n",
       " 'grabacion',\n",
       " 'efect',\n",
       " 'suen',\n",
       " 'horror',\n",
       " 'explic',\n",
       " 'larg',\n",
       " 'tem',\n",
       " 'trat',\n",
       " 'sen',\n",
       " 'a',\n",
       " 'senador',\n",
       " 'llam_atencion',\n",
       " 'ministr',\n",
       " 'viviend',\n",
       " 'mayor',\n",
       " 'propietari',\n",
       " 'casit',\n",
       " 'departament',\n",
       " 'pas',\n",
       " 'instanci',\n",
       " 'analiz',\n",
       " 'chilen',\n",
       " 'cultural',\n",
       " 'propietari',\n",
       " 'aspir',\n",
       " 'a',\n",
       " 'propietari',\n",
       " 'algui',\n",
       " 'sint',\n",
       " 'ofend',\n",
       " 'pid',\n",
       " 'disculp',\n",
       " 'cas',\n",
       " '¿cuant',\n",
       " 'person',\n",
       " 'viviend',\n",
       " 'chil',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'cifr',\n",
       " 'uso',\n",
       " 'defend',\n",
       " 'admit',\n",
       " '604',\n",
       " 'hogar',\n",
       " 'chilen',\n",
       " '35',\n",
       " 'millon_habit',\n",
       " 'viviend',\n",
       " 'acuerd',\n",
       " 'a',\n",
       " 'encuest',\n",
       " 'cas',\n",
       " '2017',\n",
       " 'consider',\n",
       " 'pag',\n",
       " 'propied',\n",
       " 'credit',\n",
       " 'hipotecari',\n",
       " '–',\n",
       " '¿y',\n",
       " 'cas',\n",
       " '–',\n",
       " 'pront',\n",
       " 'habl',\n",
       " 'departament',\n",
       " 'estacion',\n",
       " 'bodeg',\n",
       " 'cas',\n",
       " 'banc',\n",
       " 'suert',\n",
       " 'dueñ',\n",
       " 'bañ',\n",
       " 'deb',\n",
       " '16_años',\n",
       " '–',\n",
       " 'propietari',\n",
       " '–',\n",
       " 'propietari',\n",
       " 'inscrit',\n",
       " 'a',\n",
       " 'nombr',\n",
       " 'deb',\n",
       " 'deud',\n",
       " 'import',\n",
       " 'pag',\n",
       " 'mensual',\n",
       " 'line',\n",
       " 'defend',\n",
       " 'quien',\n",
       " 'critic',\n",
       " 'desapeg',\n",
       " 'realid',\n",
       " 'chilen',\n",
       " 'llev',\n",
       " 'visit',\n",
       " '80',\n",
       " 'comun',\n",
       " 'pod',\n",
       " 'palp',\n",
       " 'conoc',\n",
       " 'dram',\n",
       " 'neces',\n",
       " 'viviend',\n",
       " 'mil',\n",
       " 'famili',\n",
       " 'call',\n",
       " 'oficin',\n",
       " 'molest',\n",
       " 'critic',\n",
       " 'reconoc',\n",
       " 'escuch_entrev',\n",
       " 'complet',\n",
       " 'a',\n",
       " 'continu']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "### Definir el modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos parámetros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
    "- `window` : Tamaño de la ventana. Usaremos 4.\n",
    "- `size` : El tamaño de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero después de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
    "- `workers`: Cantidad de CPU que serán utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = Word2Vec(min_count = 10,\n",
    "                 window = 4,\n",
    "                 sample= 6e-5,\n",
    "                 alpha= 0.03,\n",
    "                 min_alpha= 0.0007,\n",
    "                 negative= 0,\n",
    "                 workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se creará un conjunto que contendrá (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.build_vocab(sentences, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuación, entenaremos el modelo. \n",
    "Los parámetros que usaremos serán: \n",
    "\n",
    "- `total_examples`: Número de documentos.\n",
    "- `epochs`: Número de veces que se iterará sobre el corpus.\n",
    "\n",
    "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2vec.train(sentences, total_examples = w2vec.corpus_count, epochs=15, report_delay=10 )\n",
    "print('time: {} mins'.format(round((time() - t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-6c363b0058da>:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2vec.init_sims(replace=True)\n",
      "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:23:19.548567Z",
     "start_time": "2020-05-07T19:23:18.572531Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./pretrained_models'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "\n",
    "w2vec.save('./pretrained_models/w2vec_noticias.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_2 = KeyedVectors.load('./pretrained_models/w2vec_noticias.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Palabras mas similares y Analogías\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la información contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matemático, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fa8aa0ef5b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alcaz', 0.36716562509536743),\n",
       " ('pon', 0.3508830666542053),\n",
       " ('instanci', 0.3440763056278229),\n",
       " ('hoffm', 0.33845919370651245),\n",
       " ('urban', 0.3383106589317322),\n",
       " ('vaticin', 0.33535242080688477),\n",
       " ('gratuit', 0.332356333732605),\n",
       " ('middl', 0.33211106061935425),\n",
       " ('imped', 0.3308682441711426),\n",
       " ('ido', 0.3300718069076538)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_2.wv.most_similar(positive=[\"social\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
