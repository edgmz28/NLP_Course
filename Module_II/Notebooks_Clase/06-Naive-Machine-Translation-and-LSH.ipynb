{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo II: Traducci√≥n autom√°tica y Local Sensitive Hashing (LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora implementaremos un sistema de traducci√≥n autom√°tica y luego\n",
    "veremos c√≥mo funcionan las funciones hash. Empecemos importando\n",
    "las funciones requeridas!\n",
    "\n",
    "\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar las librer√≠as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Embeddings para palabras en Ingl√©s y Frances\n",
    "\n",
    "Escribe un programa que traduzca del ingl√©s al franc√©s.\n",
    "\n",
    "## Los datos\n",
    "\n",
    "El conjunto de datos completo para las wordembeddings en ingl√©s es de aproximadamente 3,64 gigabytes, y el franc√©s\n",
    "son de aproximadamente 629 megabytes. Trabajaremos con un conjunto m√°s peque√±o de datos, una muestra significativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subconjunto de los datos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargue dos diccionarios que mapean las palabras del ingl√©s al franc√©s\n",
    "* Un diccionario de entrenamiento\n",
    "* y un diccionario de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viendo los diccionarios de Ingl√©s y Franc√©s\n",
    "\n",
    "* `en_fr_train` es un diccionario donde el key es la palabra en ingl√©s y el valor es la palabra en franc√©s.\n",
    "```\n",
    "{'the': 'la',\n",
    " 'and': 'et',\n",
    " 'was': '√©tait',\n",
    " 'for': 'pour',\n",
    "```\n",
    "\n",
    "* `en_fr_test` es similar que `en_fr_train`, pero este es un conjunto de prueba. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando matrices de embeddings y transformaci√≥n\n",
    "\n",
    "####  Traducci√≥n de diccionario de ingl√©s a franc√©s mediante embeddings\n",
    "\n",
    "Implementaremos la funci√≥n `get_matrices`, que toma los datos cargados y retorna las matrices `X` y `Y`.\n",
    "\n",
    "Entrada:\n",
    "- `en_fr` : diccionario del Ingl√©s a Franc√©s\n",
    "- `en_embeddings` : embeddings de palabras en ingl√©s\n",
    "- `fr_embeddings` : embeddings de palabras en Franc√©s\n",
    "\n",
    "Retorna:\n",
    "- matrices `X` y `Y`, donde cada rengl√≥n en X es la palabra embebida para una plabra en ingl√©s, y lo mismo con Y es la palabra embebida en franc√©s.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\">\n",
    "<img src='Figures/X_to_Y.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:800px;height:200px;\" /> Figure 2 </div>\n",
    "\n",
    "Utilice el diccionario `en_fr` para asegurarse de que la i-√©sima fila de la matriz` X`\n",
    "corresponde a la i-√©sima fila de la matriz \"Y\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos la funci√≥n `get_matrices ()` para obtener los conjuntos `X_train` e` Y_train`\n",
    "de los embeddings de palabras en ingl√©s y franc√©s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Traductores\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/e_to_f.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:200px;\" />  </div>\n",
    "\n",
    "Escriba un programa que traduzca palabras del ingl√©s al franc√©s utilizando word embeddings y modelos de espacio vectorial.\n",
    "\n",
    "\n",
    "##  Traducci√≥n como transformaci√≥n lineal de embeddings\n",
    "Dados los diccionarios de embeddings de palabras en ingl√©s y franc√©s, crearemos una matriz de transformaci√≥n `R`\n",
    "* Dada una palabra incrustada en ingl√©s, $ \\mathbf {e} $, puede multiplicar $ \\mathbf {eR} $ para obtener una nueva palabra incrustada $ \\mathbf {f} $.\n",
    "\n",
    "    \n",
    "* A continuaci√≥n, puede calcular los vecinos m√°s cercanos a `f` en los embeddings francesas y recomendar la palabra que es m√°s similar a los embeddings de palabras transformadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describir la traducci√≥n como el problema de minimizaci√≥n\n",
    "\n",
    "Encuentre una matriz `R` que minimice la siguiente ecuaci√≥n.\n",
    "\n",
    "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
    "\n",
    "### Norma de Frobenius\n",
    "\n",
    "La norma de Frobenius de una matriz $ A $ (asumiendo que es de dimensi√≥n $ m, n $) se define como la ra√≠z cuadrada de la suma de los cuadrados absolutos de sus elementos:\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n de perdida\n",
    "\n",
    "En las aplicaciones del mundo real, la p√©rdida de la norma Frobenius:\n",
    "\n",
    "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
    "\n",
    "a menudo se reemplaza por su valor al cuadrado dividido por $ m $:\n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "donde $ m $ es el n√∫mero de ejemplos (filas en $ \\ mathbf {X} $).\n",
    "\n",
    "* Se encuentra la misma R cuando se usa esta funci√≥n de p√©rdida en comparaci√≥n con la norma de Frobenius original.\n",
    "* La raz√≥n para tomar el cuadrado es que es m√°s f√°cil calcular el gradiente del Frobenius al cuadrado.\n",
    "* La raz√≥n para dividir entre $ m $ es que estamos m√°s interesados en la p√©rdida promedio por inserci√≥n que en la p√©rdida de todo el conjunto de entrenamiento.\n",
    "     * La p√©rdida de todo el conjunto de entrenamiento aumenta con m√°s palabras (ejemplos de entrenamiento),\n",
    "     por lo que tomar el promedio nos ayuda a rastrear la p√©rdida promedio independientemente del tama√±o del conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementaci√≥n del mecanismo de traducci√≥n\n",
    "\n",
    "#### Calculando el loss\n",
    "* La funci√≥n de p√©rdida ser√° la norma de Frobenoius al cuadrado de la diferencia entre\n",
    "matriz y su aproximaci√≥n, dividida por el n√∫mero de ejemplos de entrenamiento $ m $.\n",
    "* Su f√≥rmula es:\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "donde $a_{i j}$ es el valo de la $i$-√©simo rengl√≥n y $j$-√©sima columna de la matriz $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calcular la aproximaci√≥n de `Y` mediante la matriz multiplicando` X` y `R`\n",
    "* Calcular la diferencia `XR - Y`\n",
    "* Calcular la norma de Frobenius al cuadrado de la diferencia y div√≠dala por $ m $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Calculando el gradiente de la funci√≥n loss con respecto a la matriz de transforaci√≥n R\n",
    "\n",
    "* Calcular el gradiente de la p√©rdida con respecto a la matriz de transformaci√≥n \"R\".\n",
    "* El gradiente nos da la direcci√≥n en la que debemos disminuir `R`\n",
    "para minimizar la p√©rdida.\n",
    "* $ m $ es el n√∫mero de ejemplos de entrenamiento (n√∫mero de filas en $ X $).\n",
    "* La f√≥rmula para el gradiente de la funci√≥n de p√©rdida $ ùêø (ùëã, ùëå, ùëÖ) $ es:\n",
    "\n",
    "$$\\frac{d}{dR}ùêø(ùëã,ùëå,ùëÖ)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar la R √≥ptima con el algoritmo de descenso de gradiente\n",
    "\n",
    "#### Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudoc√≥digo:\n",
    "1. Calcular el gradiente $g$ del loss con respecto a la matriz $R$.\n",
    "2. Update $R$ con la formula:\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "Donde $\\alpha$ es el learning rate, que es un escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "\n",
    "* La tasa de aprendizaje o \"tama√±o de paso\" $ \\ alpha $ es un coeficiente que decide cu√°nto queremos cambiar $ R $ en cada paso.\n",
    "* Si cambiamos $ R $ demasiado, podr√≠amos saltarnos el √≥ptimo dando un paso demasiado grande.\n",
    "* Si solo hacemos peque√±os cambios en $ R $, necesitaremos muchos pasos para alcanzar el √≥ptimo.\n",
    "* La tasa de aprendizaje $ \\ alpha $ se usa para controlar esos cambios.\n",
    "* Los valores de $ \\ alpha $ se eligen dependiendo del problema, y usaremos `learning_rate` $ = 0.0003 $ como valor predeterminado para nuestro algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clacular la matriz de transformaci√≥n R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probando el traductor\n",
    "\n",
    "### Algoritmo k-Nearest neighbors \n",
    "\n",
    "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
    "* k-NN es un m√©todo que toma un vector como entrada y encuentra los otros vectores en el conjunto de datos que est√°n m√°s cerca de √©l.\n",
    "* La 'k' es el n√∫mero de \"vecinos m√°s cercanos\" a encontrar (por ejemplo, k = 2 encuentra los dos vecinos m√°s cercanos).\n",
    "\n",
    "### Buscando el embedding de la traducci√≥n \n",
    "Dado que estamos aproximando la funci√≥n de traducci√≥n de los embeddings de ingl√©s a franc√©s mediante una matriz de transformaci√≥n lineal $ \\mathbf {R} $, la mayor√≠a de las veces no obtendremos la incrustaci√≥n exacta de una palabra francesa cuando transformamos los embeddings $ \\mathbf { e} $ de alguna palabra en ingl√©s en particular en el espacio de embeddings franc√©s.\n",
    "* ¬°Aqu√≠ es donde $ k $ -NN se vuelve realmente √∫til! Al usar $ 1 $ -NN con $ \\mathbf {eR} $ como entrada, podemos buscar un embedding $ \\mathbf {f} $ (como una fila) en la matriz $ \\mathbf {Y} $ que es la m√°s cercana a el vector transformado $ \\mathbf {eR} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridad por  Coseno\n",
    "Similitud de coseno entre los vectores $ u $ y $ v $ calculada como el coseno del √°ngulo entre ellos.\n",
    "La formula es\n",
    "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
    "\n",
    "* $\\cos(u,v)$ = $1$ cuando $u$ y $v$ se encuentran en la misma l√≠nea y tienen la misma direcci√≥n.\n",
    "* $\\cos(u,v)$ es $-1$ Cuando ellas tienen direcciones exactamente opuestas.\n",
    "* $\\cos(u,v)$ es $0$ cuando los vectores son ortogonales (perpendiculares) entre s√≠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota: La distancia y la similitud son cosas bastante opuestas.\n",
    "* Podemos obtener la m√©trica de la distancia a partir de la similitud del coseno, pero la similitud del coseno no se puede usar directamente como la m√©trica de la distancia.\n",
    "* Cuando la similitud del coseno aumenta (hacia $ 1 $), la \"distancia\" entre los dos vectores disminuye (hacia $ 0 $).\n",
    "* Podemos definir la distancia del coseno entre $ u $ y $ v $ como\n",
    "$$ d_{\\text {cos}} (u, v) = 1- \\cos(u, v) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completando la funci√≥n `nearest_neighbor()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your translation and compute its accuracy\n",
    "\n",
    "* Calculando el accuracy como $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# LSH y b√∫squeda de documentos\n",
    "\n",
    "* Procesar los tweets y representar cada tweet como un vector (representar un\n",
    "documento con un embedding vector).\n",
    "* Utilice hashing y k vecinos m√°s cercanos para encontrar tweets\n",
    "que son similares a un tweet determinado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Obteniendo los embeddings de los documentos\n",
    "\n",
    "#### Modelo de Bag-of-words (BOW) \n",
    "\n",
    "Los documentos de texto son secuencias de palabras.\n",
    "* El orden de las palabras marca la diferencia. Por ejemplo, las frases \"La tarta de manzana es mejor que la pizza de pepperoni \"y\" La pizza de pepperoni es mejor que la tarta de manzana \"\n",
    "tienen significados opuestos debido al orden de las palabras.\n",
    "* Sin embargo, para algunas aplicaciones, ignorar el orden de las palabras puede permitir\n",
    "nosotros para formar un modelo eficiente y a√∫n eficaz.\n",
    "* Este enfoque se denomina modelo de documento de bolsa de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Guardar los vectores de documento en un diccionario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Buscando los tweets\n",
    "\n",
    "Ahora tenemos un vector de dimensi√≥n (m, d) donde `m` es el n√∫mero de tweets\n",
    "(10,000) y `d` es la dimensi√≥n de los embeddings (300). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-3\"></a>\n",
    "\n",
    "## Buscando los tweets m√°s similares con LSH\n",
    "\n",
    "Ahora implementaremos el hashing (LSH) para identificar el tweet m√°s similar.\n",
    "* En lugar de mirar los 10,000 vectores, puede buscar un subconjunto para encontrar\n",
    "sus vecinos m√°s cercanos.\n",
    "\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/one.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 3 </div>\n",
    "\n",
    "Puede dividir el espacio vectorial en regiones y buscar dentro de una regi√≥n los vecinos m√°s cercanos de un vector dado.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/four.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 4 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escogiendo los n√∫mero de planos\n",
    "\n",
    "* Cada plano divide el espacio en $ 2 $ partes.\n",
    "* Entonces $ n $ aviones dividen el espacio en $ 2 ^ {n} $ cubos de hash.\n",
    "* Queremos organizar 10,000 vectores de documentos en dep√≥sitos para que cada dep√≥sito tenga vectores de $ ~ 16 $.\n",
    "* Para eso necesitamos $ \\frac {10000} {16} = 625 $ cubos.\n",
    "* Estamos interesados en $ n $, n√∫mero de aviones, por lo que $ 2 ^ {n} = 625 $. Ahora, podemos calcular $ n = \\log_{2} 625 = 9.29 \\approx 10 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Obteniendo el n√∫mero para el vector hash\n",
    "\n",
    "Para cada vector, necesitamos obtener un n√∫mero √∫nico asociado a ese vector para poder asignarlo a un \"bucket de hash\".\n",
    "\n",
    "### Hyperlanes in vector spaces\n",
    "* En un espacio vectorial dimensional de $ 3 $, el hiperplano es un plano regular. En el espacio vectorial dimensional de $ 2 $, el hiperplano es una l√≠nea.\n",
    "* Generalmente, el hiperplano es un subespacio que tiene una dimensi√≥n $ 1 $ menor que la del espacio vectorial original.\n",
    "* Un hiperplano se define de forma √∫nica por su vector normal.\n",
    "* El vector normal $ n $ del plano $ \\ pi $ es el vector al que todos los vectores en el plano $ \\ pi $ son ortogonales (perpendiculares en el caso dimensional de $ 3 $).\n",
    "\n",
    "### Usando Hiperplanos para cortar el espacio vectorial\n",
    "\n",
    "Podemos usar un hiperplano para dividir el espacio vectorial en $ 2 $ partes.\n",
    "* Todos los vectores cuyo producto escalar con el vector normal de un plano es positivo est√°n en un lado del plano.\n",
    "* Todos los vectores cuyo producto escalar con el vector normal del plano es negativo est√°n en el otro lado del plano.\n",
    "\n",
    "### Encodeando los buckets hash\n",
    "* Para un vector, podemos tomar su producto escalar con todos los planos, luego codificar esta informaci√≥n para asignar el vector a un solo cubo hash.\n",
    "* Cuando el vector apunta al lado opuesto del hiperplano de lo normal, codif√≠quelo con 0.\n",
    "* De lo contrario, si el vector est√° en el mismo lado que el vector normal, codif√≠quelo por 1.\n",
    "* Si calcula el producto escalar con cada plano en el mismo orden para cada vector, ha codificado el ID de hash √∫nico de cada vector como un n√∫mero binario, como [0, 1, 1, ... 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando los hash buckets\n",
    "\n",
    "Inicializamos los hash de la tabla. Es una lista de matrices `N_UNIVERSES`, cada una describe su propia tabla hash. Cada matriz tiene filas `N_DIMS` y columnas` N_PLANES`. Cada columna de esa matriz es un vector normal dimensional `N_DIMS` para cada uno de los hiperplanos` N_PLANES` que se utilizan para crear cubos de la tabla hash particular.\n",
    "\n",
    "* Primero multiplica tu vector `v`, con un plano correspondiente. Esto le dar√° un vector de dimensi√≥n $ (1, \\text{N_planes}) $.\n",
    "* Luego, convertir√° todos los elementos de ese vector en 0 o 1.\n",
    "* Creas un vector hash haciendo lo siguiente: si el elemento es negativo, se convierte en un 0; de lo contrario, lo cambias a un 1.\n",
    "* Luego calcula el n√∫mero √∫nico para el vector iterando sobre `N_PLANES`\n",
    "* Luego multiplica $ 2^i $ por el bit correspondiente (0 o 1).\n",
    "* Luego almacenar√° esa suma en la variable `hash_value`.\n",
    "\n",
    "Cree un hash para el vector en la siguiente funci√≥n.\n",
    "Utilice esta f√≥rmula:\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crea los conjuntos de planos\n",
    "* Cree m√∫ltiples (25) conjuntos de planos (los planos que dividen la regi√≥n).\n",
    "* Puede pensar en estos como 25 formas distintas de dividir el espacio vectorial con un conjunto diferente de planos.\n",
    "* Cada elemento de esta lista contiene una matriz con 300 filas (la palabra vector tiene 300 dimensiones) y 10 columnas (hay 10 planos en cada \"universo\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Creando la Tabla Hash\n",
    "\n",
    "Dado que ya tenemos una representaci√≥n para cada vector (o tweet), ahora crearemos una tabla hash. Necesitamos una tabla hash, de modo que, dado un hash_id, pueda buscar r√°pidamente los vectores correspondientes. Esto le permite reducir su b√∫squeda por una cantidad de tiempo significativa.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/table.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px;\" />  </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creando todas las tablas hash \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate K-NN\n",
    "\n",
    "\n",
    "Implementar aproximadamente K vecinos m√°s cercanos utilizando hash sensible a la localidad,\n",
    "para buscar documentos que sean similares a un documento dado en el\n",
    "√≠ndice `doc_id`.\n",
    "\n",
    "##### entradas\n",
    "* `doc_id` es el √≠ndice de la lista de documentos` all_tweets`.\n",
    "* `v` es el vector de documento para el tweet en` all_tweets` en el √≠ndice `doc_id`.\n",
    "* `planes_l` es la lista de planos (la variable global creada anteriormente).\n",
    "* `k` es el n√∫mero de vecinos m√°s cercanos a buscar.\n",
    "* `num_universes_to_use`: para ahorrar tiempo, podemos usar menos que el total\n",
    "n√∫mero de universos disponibles. De forma predeterminada, est√° configurado en `N_UNIVERSES`,\n",
    "que es $ 25 $ para esta asignaci√≥n.\n",
    "\n",
    "La funci√≥n `approx_knn` encuentra un subconjunto de vectores candidatos que\n",
    "est√°n en el mismo \"dep√≥sito de hash\" que el vector de entrada 'v'. Entonces se realiza\n",
    "los k vecinos m√°s cercanos habituales buscan en este subconjunto (en lugar de buscar\n",
    "a trav√©s de los 10,000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
