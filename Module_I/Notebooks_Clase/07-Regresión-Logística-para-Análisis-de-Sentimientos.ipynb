{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Regresión Logística para Análisis de Sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import getcwd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "from utilss import Utilities as prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepara los datos\n",
    "* `twitter_samples` contiene subconjuntos de 5,000 tweets positivos, 5,000 tweets negativos y el conjunto completo de 10,000 tweets.\n",
    "     * Si utiliza los tres conjuntos de datos, introduciríamos duplicados de los tweets positivos y negativos.\n",
    "     * Seleccionará solo los cinco mil tweets positivos y los cinco mil tweets negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train test split: 20% para test, y 80% para train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "print(len(test_pos))\n",
    "print(len(train_pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "print(len(test_neg))\n",
    "print(len(train_neg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creear una matriz de etiquetas positivas y negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Utilities Constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11340"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el diccionario de frecuencias\n",
    "prep_obj = prep()\n",
    "\n",
    "\n",
    "freqs = prep_obj.build_freqs(train_x, train_y)\n",
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento del tweet\n",
    "\n",
    "La función dada `process_tweet ()` tokeniza el tweet en palabras individuales, elimina las palabras vacías y aplica la derivación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@leomanaids_ look here u little shit :-) I fell asleep &amp; then woke up &amp; thought u were asleep :-) PLUS this was earlier :-) so fuk u dude\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['look',\n",
       " 'u',\n",
       " 'littl',\n",
       " 'shit',\n",
       " ':-)',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " 'woke',\n",
       " 'thought',\n",
       " 'u',\n",
       " 'asleep',\n",
       " ':-)',\n",
       " 'plu',\n",
       " 'earlier',\n",
       " ':-)',\n",
       " 'fuk',\n",
       " 'u',\n",
       " 'dude']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x[1234])\n",
    "prep_obj.process_tweet(train_x[1234])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extrayendo las características\n",
    "\n",
    "* Dada una lista de tweets, extraiga las características y guárdelas en una matriz. Extraerás dos características.\n",
    "     * La primera característica es la cantidad de palabras positivas en un tweet.\n",
    "     * La segunda característica es la cantidad de palabras negativas en un tweet.\n",
    "* Luego entrene su clasificador de regresión logística en estas características.\n",
    "* Pruebe el clasificador en un conjunto de validación. \n",
    "\n",
    "### Implementación de la función de extract_features.\n",
    "* Esta función admite un solo tweet.\n",
    "* Procesaremos el tweet usando la función `process_tweet()` importada y la guardaremos en la lista de palabras del tweet.\n",
    "* Recorreremos cada palabra en la lista de palabras procesadas\n",
    "     * Para cada palabra, consultaremos el diccionario `freqs` para el recuento cuando esa palabra tiene una etiqueta positiva '1'. (con clave (palabra, 1.0)\n",
    "     * Hacemos lo mismo con el recuento para cuando la palabra esté asociada con la etiqueta negativa '0'. (con la clave (palabra, 0.0).)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    \n",
    "    word_l = prep_obj.process_tweet(tweet)\n",
    "    \n",
    "    x= np.zeros((1,3))\n",
    "    \n",
    "    x[0,0]=1\n",
    "    \n",
    "    # y = b_0 + b_1 * x_1 + b_2 * x_2\n",
    "    # X= [1 ;x_1; x2]\n",
    "    # B=[b_0; b1; b2]\n",
    "    \n",
    "    # y = B*X\n",
    "    \n",
    "    for word in word_l:\n",
    "        \n",
    "        # incrementar el conteo de palabras para la etiqueta positiva (1)\n",
    "        x[0,1] += freqs.get((word,1),0)\n",
    "        \n",
    "        # incrementar el conteo de palabras para la etiqueta negativa (0)\n",
    "        x[0,2] += freqs.get((word,0),0)\n",
    "        \n",
    "    return x \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpl = extract_features(train_x[1234], freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 2.239e+03 6.800e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de la Regresión Logística\n",
    "\n",
    "\n",
    "### Función sigmoide\n",
    "Aprenderá a utilizar la regresión logística para la clasificación de texto.\n",
    "* La función sigmoidea se define como:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "Asigna la entrada 'z' a un valor que varía entre 0 y 1, por lo que puede tratarse como una probabilidad.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926084586557181"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(4.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2689414213699951"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression y función sigmoide\n",
    "\n",
    "La regresión logística toma una regresión lineal regular y aplica un sigmoide a la salida de la regresión lineal.\n",
    "\n",
    "Regresion:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Tenga en cuenta que los valores $ \\theta $ son \"pesos\". Si realizó la especialización en aprendizaje profundo, nos referimos a los pesos con el vector `w`. En este curso, usamos una variable diferente $ \\theta $ para referirnos a los pesos.\n",
    "\n",
    "Regresión logística\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Nos referiremos a 'z' como los 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de costo y gradiente\n",
    "\n",
    "La función de costo utilizada para la regresión logística es el promedio de la pérdida de registro en todos los ejemplos de entrenamiento:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ es la cantidad de ejemplos de entrenamiento\n",
    "* $y^{(i)}$ es la etiqueta real del i-ésimo dato de entrenamiento.\n",
    "* $h(z(\\theta)^{(i)})$ es la predicción del modelo para el i-ésimo ejemplo de entrenamiento.\n",
    "\n",
    "La función de pérdida para un solo ejemplo de entrenamiento es\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* Todos los valores de $ h $ están entre 0 y 1, por lo que los registros serán negativos. Esa es la razón del factor -1 aplicado a la suma de los dos términos de pérdida.\n",
    "* Tenga en cuenta que cuando el modelo predice 1 ($ h (z (\\theta)) = 1 $) y la etiqueta $ y $ también es 1, la pérdida para ese ejemplo de entrenamiento es 0.\n",
    "* De manera similar, cuando el modelo predice 0 ($ h (z (\\theta)) = 0 $) y la etiqueta real también es 0, la pérdida para ese ejemplo de entrenamiento es 0.\n",
    "* Sin embargo, cuando la predicción del modelo es cercana a 1 ($ h (z (\\theta)) = 0.9999 $) y la etiqueta es 0, el segundo término de la pérdida logarítmica se convierte en un gran número negativo, que luego se multiplica por el factor general de -1 para convertirlo en un valor de pérdida positivo. $ -1 \\times (1 - 0) \\times log (1 - 0.9999) \\approx 9.2 $ Cuanto más se acerque la predicción del modelo a 1, mayor será la pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Del mismo modo, si el modelo predice cerca de 0 ($ h (z) = 0.0001 $) pero la etiqueta real es 1, el primer término en la función de pérdida se convierte en un número grande: $ -1 \\times log (0.0001) \\approx 9.2 $. Cuanto más cercana sea la predicción a cero, mayor será la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizar los pesos\n",
    "\n",
    "Para actualizar su vector de peso $ \\theta $, aplicará el descenso de gradiente para mejorar iterativamente las predicciones de su modelo.\n",
    "El gradiente de la función de costo $ J $ con respecto a uno de los pesos $ \\theta_j $ es:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' es el índice de todos los ejemplos de formación \"m\".\n",
    "* 'j' es el índice del peso $ \\theta_j $, entonces $ x_j $ es la característica asociada con el peso $ \\theta_j $\n",
    "\n",
    "* Para actualizar el peso $ \\theta_j $, lo ajustamos restando una fracción del gradiente determinado por $ \\alpha $:\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j} J (\\theta) $$\n",
    "* La tasa de aprendizaje $ \\alpha $ es un valor que elegimos para controlar qué tan grande será una sola actualización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de la función Gradiente Descendente\n",
    "* El número de iteraciones `num_iters` es el número de veces que utilizará todo el conjunto de entrenamiento.\n",
    "* Para cada iteración, calculará la función de costo usando todos los ejemplos de entrenamiento (hay ejemplos de entrenamiento `m`), y para todas las funciones.\n",
    "* En lugar de actualizar un solo peso $ \\theta_i $ a la vez, podemos actualizar todos los pesos en el vector de columna:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $ \\mathbf {\\theta} $ tiene dimensiones (n + 1, 1), donde 'n' es el número de características, y hay un elemento más para el término de sesgo $ \\theta_0 $ (tenga en cuenta que el valor de característica correspondiente $ \\mathbf {x_0} $ es 1).\n",
    "* Los 'logits', 'z', se calculan multiplicando la matriz de características 'x' con el vector de peso 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* La predicción 'h' se calcula aplicando el sigmoide a cada elemento en 'z': $ h (z) = sigmoid (z) $, y tiene dimensiones (m, 1).\n",
    "* La función de costo $ J $ se calcula tomando el producto escalar de los vectores 'y' y 'log (h)'. Dado que tanto 'y' como 'h' son vectores de columna (m, 1), transponga el vector a la izquierda, de modo que la multiplicación de matrices de un vector de fila con un vector de columna realice el producto escalar.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* La actualización de theta también está vectorizada. Debido a que las dimensiones de $ \\mathbf {x} $ son (m, n + 1), y tanto $ \\mathbf {h} $ como $ \\mathbf {y} $ son (m, 1), necesitamos transponer $ \\mathbf {x} $ y colóquelo a la izquierda para realizar la multiplicación de matrices, que luego da la respuesta (n + 1, 1) que necesitamos:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(), np.log(1-h)))\n",
    "        \n",
    "        theta = theta -(alpha/m)*np.dot(x.transpose(),(h-y))\n",
    "        \n",
    "        \n",
    "    J = float(J)\n",
    "    \n",
    "    return J, theta     \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 2.239e+03, 6.800e+02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_x)):\n",
    "    X[i,:] = extract_features(train_x[i], freqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.020e+03, 6.100e+01],\n",
       "       [1.000e+00, 3.573e+03, 4.440e+02],\n",
       "       [1.000e+00, 3.005e+03, 1.150e+02],\n",
       "       ...,\n",
       "       [1.000e+00, 1.440e+02, 7.830e+02],\n",
       "       [1.000e+00, 2.050e+02, 3.890e+03],\n",
       "       [1.000e+00, 1.890e+02, 3.974e+03]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.zeros((3,1))\n",
    "alpha = 1e-9\n",
    "niter= 1500\n",
    "\n",
    "J, theta = gradientDescent(X, Y, theta_0, alpha, niter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24216476596262507"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.25227840e-08],\n",
       "       [ 5.23899773e-04],\n",
       "       [-5.55170736e-04]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thetha[0] + theta[1]*x1 + theta[2]*x2\n",
    "\n",
    "tweet_ejemplo=\"great week\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51836744]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tweet(tweet_ejemplo,freqs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = ['happy','happy weekend', 'buena inundacion', 'nice work asshole', 'bad', 'this movie was very bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy -> 0.518580\n",
      "happy weekend -> 0.523646\n",
      "buena inundacion -> 0.500000\n",
      "nice work asshole -> 0.504301\n",
      "bad -> 0.494339\n",
      "this movie was very bad -> 0.494206\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweet_list:\n",
    "    print('%s -> %f' % (tweet, predict_tweet(tweet,freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
