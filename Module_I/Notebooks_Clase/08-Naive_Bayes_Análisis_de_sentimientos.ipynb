{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Naive Bayes para Análisis de Sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aplicaremos Naive Bayes para el análisis de sentimientos de tweets, lo que haremos en este notebook será:\n",
    "\n",
    "* Entrenar un modelo de Naive Bayes para el análisis de sentimientos\n",
    "* Probaremos el modelo creado\n",
    "* Calcularemos las proporciones de palabras positivas a palabras negativas\n",
    "* Veremos una función de análisis de errores\n",
    "* Predeciremos un tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aún no se han descargado:\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### añadir una carpeta tmp2, de nuestro workspace local que contenga las dependencias instaladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos\n",
    "\n",
    "Para cualquier proyecto de aprendizaje automático, una vez que se haya recopilado los datos, el primer paso es el preprocesamiento.\n",
    "\n",
    "- **Eliminar el ruido**: eliminar las palabras que no dicen mucho sobre el contenido. Estos incluyen todas las palabras comunes como 'I, you, are, is, etc...' que no nos darían suficiente información sobre el sentimiento.\n",
    "- También eliminaremos los tickers del mercado de valores, los símbolos de retuits, los hipervínculos y los hashtags porque no pueden brindarle mucha información sobre el sentimiento.\n",
    "- También hay que eliminar toda la puntuación de un tweet. La razón para hacer esto es porque queremos tratar las palabras con o sin puntuación como la misma palabra, en lugar de tratar las palabras \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" como palabras diferentes.\n",
    "- Por último, desea utilizaremos la técnica de stemming para realizar un seguimiento de una sola variación de cada palabra. En otras palabras, trataremos \"motivation\", \"motivated\", y \"motivate\" de manera similar agrupándolos dentro de la misma raíz de \"motiv-\".\n",
    "\n",
    "Recuerden que ya tenemos una función para esto (`process_tweet()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de ayuda\n",
    "\n",
    "Para entrenar el modelo, necesitaremos construir un diccionario donde las claves sean una tupla (word, label) y los valores sean la frecuencia correspondiente. Tenga en cuenta que las etiquetas que usaremos aquí son 1 para positivo y 0 para negativo.\n",
    "\n",
    "Implementaremos la función `lookup()` que toma del diccionario `freqs` una palabra y una etiqueta (1 or 0) y retorna el número de veces que la palabra y esa etiqueta aparecen en la collección de tweets.\n",
    "\n",
    "Por ejemplo: Dado una lista de sentencias en los tweets: `[\"i am rather excited\", \"you are rather happy\"]` y la etiqueta 1, la función regresara el diccionario que contiene lo siguiente:\n",
    "\n",
    "{\n",
    "    (\"rather\", 1): 2\n",
    "    (\"happi\", 1) : 1\n",
    "    (\"excit\", 1) : 1\n",
    "}\n",
    "\n",
    "- Observe cómo para cada palabra en la cadena dada, se asigna la misma etiqueta 1 a cada palabra.\n",
    "- Observe cómo las palabras \"i\" y \"am\" no se guardan, ya que fue eliminada por process_tweet porque es una stopword.\n",
    "- Observe cómo la palabra \"rather\" aparece dos veces en la lista de tweets, por lo que su valor de recuento es 2.\n",
    "\n",
    "Crearemos una función `count_tweets()` que toma una lista de tweets como entrada, las limpia, y regresa un diccionario.\n",
    "\n",
    "- La clave en el diccionario es una tupla que contiene la palabra despues del stemming y su etiqueta de clase, Ej. (\"happi\", 1).\n",
    "- El valor del key, será la cantidad de veces que esta palabra aparece en la colección de tweets dada (un número entero).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo usando Naive Bayes\n",
    "\n",
    "Naive bayes es un algoritmo que se puede usar para aplicaciones como el análisis de sentimientos. Este algoritmo poco tiempo en entrenarse pero tiene un tiempo de predicción corto.\n",
    "\n",
    "\n",
    "#### ¿Cómo entrenamos un clasificador con Naive Bayes ?\n",
    "\n",
    "- La primera parte del entrenamiento de un clasificador Naive Bayes es identificar el número de clases que tiene.\n",
    "- Crearemos una probabilidad para cada clase.\n",
    "\n",
    "\n",
    "$P(D_{pos})$ denota la probabilidad que el documento sea positivo.\n",
    "$P(D_{neg})$ denota la probabilidad que el documento sea negativo.\n",
    "\n",
    "Utilizaremos las fórmulas de la siguiente manera y almacenaremos los valores en un diccionario:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Donde $ D $ es el número total de documentos, o tweets en este caso, $ D_ {pos} $ es el número total de tweets positivos y $ D_ {neg} $ es el número total de tweets negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior y Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "\n",
    "La probabilidad previa (prior) representa la probabilidad subyacente de la población (tweets) objetivo de que un tweet sea positivo frente a uno negativo. En otras palabras, si no tuviéramos información específica y seleccionáramos a ciegas un tweet del conjunto de la población, ¿cuál es la probabilidad de que sea positivo frente a que sea negativo? Ese es el \"previo\".\n",
    "\n",
    "El \"prior\" es el ratio de las probabilidades $\\frac{P(D_{pos})}{P(D_{neg})}$. Podemos tomar el logaritmo del \"prior\" para cambiar su escala, y lo llamaremos logprior.\n",
    "\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note que $log(\\frac{A}{B})$ es igual a $log(A) - log(B)$.  Por lo tanto logprior puede calcularse como la diferencia de dos logaritmos (propiedad de logaritmos).\n",
    "\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilidad positiva y negativa de una palabra\n",
    "Para calcular la probabilidad positiva y la probabilidad negativa de una palabra específica en el vocabulario, usaremos las siguientes entradas:\n",
    "\n",
    "- $freq_{pos}$ y $freq_{neg}$ son las frecuencias de esa palabra específica en la clase positiva o negativa. En otras palabras, la frecuencia positiva de una palabra es el número de veces que se cuenta la palabra con la etiqueta 1.\n",
    "- $ N_ {pos} $ y $ N_ {neg} $ son el número total de palabras positivas y negativas para todos los documentos (para todos los tweets), respectivamente.\n",
    "\n",
    "- $V$ es el número de palabras únicas en todo el conjunto de documentos, para todas las clases, ya sean positivas o negativas.\n",
    "\n",
    "Los usaremos para calcular la probabilidad positiva y negativa de una palabra específica usando la fórmula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Note que añadimos \"+1\" en el numerador para el smoothing aditivo.  Este [artículo](https://en.wikipedia.org/wiki/Additive_smoothing) explica detalladamente sobre el smoothing aditivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "Para calcular la loglikelihood (probabilidad mínima) de esa misma palabra, podemos implementar las siguientes ecuaciones:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creación del diccionario de  frecuencias (`freqs` )\n",
    "- Dada la función `count_tweets ()`, podemos calcular un diccionario llamado `freqs` que contiene todas las frecuencias.\n",
    "- En este diccionario `freqs`, la clave es la tupla (word,label)\n",
    "- El valor es el número de veces que ha aparecido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline para la creación de la función  `train_naive_bayes`\n",
    "Dado un diccionario de frecuencias, `train_x` (una lista de tweets) y un` train_y` (una lista de etiquetas para cada tweet), implementaremos un clasificador Naive Bayes.\n",
    "\n",
    "##### Calcular $V$\n",
    "- Podemos calcular el número de palabras únicas que aparecen en el diccionario `freqs` para obtener $ V $ (usando la función` set`).\n",
    "\n",
    "##### Calcular $freq_{pos}$ y $freq_{neg}$\n",
    "- Usando el diccionario `freqs`, se puede calcular la frecuencia positiva y negativa de cada palabra $ freq_ {pos} $ y $ freq_ {neg} $.\n",
    "\n",
    "##### Calcular $N_{pos}$ y $N_{neg}$\n",
    "- Usando el diccionario `freqs`, también puede calcular el número total de palabras positivas y el número total de palabras negativas $ N_ {pos} $ y $ N_ {neg} $.\n",
    "\n",
    "##### Calcular $D$, $D_{pos}$, $D_{neg}$\n",
    "- Usando la lista de etiquetas de entrada `train_y`, calcularemos el número de documentos (tweets) $ D $, así como el número de documentos positivos (tweets) $ D_ {pos} $ y el número de documentos negativos (tweets) $ D_ { neg} $.\n",
    "- Calcularemos la probabilidad de que un documento (tweet) sea positivo $ P (D_ {pos}) $, y la probabilidad de que un documento (tweet) sea negativo $ P (D_ {neg}) $\n",
    "\n",
    "##### Calcularemos el logprior\n",
    "-  $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- Finalmente, iteramos sobre cada palabra en el vocabulario, usaremos la función `lookup` para obtener las frecuencias positivas, $ freq_ {pos} $, y las frecuencias negativas, $ freq_ {neg} $, para esa palabra específica.\n",
    "- Calcularemos la probabilidad positiva de cada palabra $ P (W_ {pos}) $, la probabilidad negativa de cada palabra $ P (W_ {neg}) $ usando las ecuaciones 4 y 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** Usaremos un diccionario para almacenar los loglikelihoods de cada palabra. La clave será la palabra, el valor será la probabilidad logarítmica de esa palabra).\n",
    "\n",
    "- se puede calcular la loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando el modelo de Naive Bayes\n",
    "\n",
    "Ahora que tenemos `logprior` y` loglikelihood`, podemos probar el modelo de Naive Bayes para predecir algunos tweets.\n",
    "\n",
    "#### Implementación de  `naive_bayes_predict`\n",
    "\n",
    "Implementaremos la función `naive_bayes_predict` para hacer predicciones de tweets.\n",
    "\n",
    "* La función toma como entrada el `tweet`,` logprior`, y `loglikelihood`.\n",
    "* Devuelve la probabilidad de que el tweet pertenezca a la clase positiva o negativa.\n",
    "* Para cada tweet, se sumarán las probabilidades de cada palabra en el tweet.\n",
    "* También se agregará el logprior de esta suma para obtener la predicción de sentimiento del tweet\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Nota\n",
    "\n",
    "Tenga en cuenta que calculamos el \"prior\" a partir de los datos de entrenamiento y que los datos de entrenamiento se dividen uniformemente entre etiquetas positivas y negativas (4000 tweets positivos y 4000 negativos). Esto significa que el ratio de positivo a negativo 1, y el logprior es 0.\n",
    "\n",
    "El valor de 0.0 significa que cuando agregamos el logprior al likelihood, simplemente estamos agregando un valor de cero a la probabilidad logarítmica. Sin embargo, recuerde incluir el logprior, porque siempre que los datos no estén perfectamente equilibrados, el logprior será un valor distinto de cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementando  test_naive_bayes\n",
    "\n",
    "* Implementaremos `test_naive_bayes` la función para ver el accuracy de las predicciones. \n",
    "* La función toma de `test_x`, `test_y`,el log_prior, y el loglikelihood\n",
    "* Regresa el ccuracy del modelo\n",
    "* Primeramente, usaremos la función `naive_bayes_predict` para hacer las predicciones para cada tweet en text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sample= ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir un tweet propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado de palabras por ratios positivos y negativos\n",
    "\n",
    "- Algunas palabras tienen un conteo de etiquetas más positivos que otras y pueden considerarse \"más positivas\". Asimismo, algunas palabras pueden considerarse más negativas que otras.\n",
    "\n",
    "- Una forma de definir el nivel de positividad o negatividad, sin calcular la probabilidad logarítmica, es comparar la frecuencia positiva con la negativa de la palabra.\n",
    "\n",
    "    - Tenga en cuenta que también podemos utilizar los cálculos de probabilidad logarítmica para comparar la positividad o negatividad relativa de las palabras.\n",
    "    \n",
    "- Podemos calcular el ratio de las frecuencias positivas y negativas de una palabra.\n",
    "- Una vez que podamos calcular estos ratios, también podemos filtrar un subconjunto de palabras que tengan una proporción mínima de positividad / negatividad o superior.\n",
    "- De manera similar, también podemos filtrar un subconjunto de palabras que tienen una proporción máxima de positividad / negatividad o menor (palabras que son menos negativas, o incluso más negativas que un umbral dado).\n",
    "\n",
    "#### Implementación `get_ratio()`\n",
    "\n",
    "- Dado el diccionario de palabras `freqs` y una palabra en particular, usaremos` lookup (freqs, word, 1) `para obtener el recuento positivo de la palabra.\n",
    "- De manera similar, usaremos la función `lookup ()` para obtener el recuento negativo de esa palabra.\n",
    "- Calcularemos la proporción de positivo dividido por conteos negativos.\n",
    "\n",
    "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
    "\n",
    "Where pos_words and neg_words correspond to the frequency of the words in their respective classes. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Words</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        Positive word count\n",
    "        </td>\n",
    "         <td>\n",
    "        Negative Word Count\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        glad\n",
    "        </td>\n",
    "         <td>\n",
    "        41\n",
    "        </td>\n",
    "    <td>\n",
    "        2\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        arriv\n",
    "        </td>\n",
    "         <td>\n",
    "        57\n",
    "        </td>\n",
    "    <td>\n",
    "        4\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :(\n",
    "        </td>\n",
    "         <td>\n",
    "        1\n",
    "        </td>\n",
    "    <td>\n",
    "        3663\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :-(\n",
    "        </td>\n",
    "         <td>\n",
    "        0\n",
    "        </td>\n",
    "    <td>\n",
    "        378\n",
    "        </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación `get_words_by_threshold(freqs,label,threshold)`\n",
    "\n",
    "* Si establecemos label en 1, buscaremos todas las palabras cuyo umbral de positivo / negativo sea mayoy o igual al umbral.\n",
    "* Si establecemos la etiqueta en 0, buscaremos todas las palabras cuyo umbral de positivo / negativo sea menor o igual al umbral.\n",
    "* Utilizaremos la función `get_ratio ()` para obtener un diccionario que contenga el recuento positivo, el recuento negativo y la proporción de recuentos positivos y negativos.\n",
    "* Agregaremos un diccionario a una lista, donde la clave es la palabra y el diccionario es el diccionario `pos_neg_ratio` que es devuelto por la función` get_ratio () `.\n",
    "\n",
    "Un ejemplo de este diccionario sería:\n",
    "```\n",
    "{'happi':\n",
    "    {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe la diferencia entre los ratios positivos y negativos. Emojis como :( y palabras como 'me' tienden a tener una connotación negativa. Otras palabras como 'glad', 'comunity' y 'arrives' tienden a encontrarse en los tw-eets positivos.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Error\n",
    "\n",
    "En esta parte, veremos algunos tweets que el modelo no clasificó correctamente. ¿Por qué crees que ocurrieron las clasificaciones erróneas? ¿Hubo alguna suposición hecha por el modelo de Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predice con tu propio tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "my_tweet = 'I am happy because I am in the best course of NLP :) #amazing'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. See you next week!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "schema_names": [
    "NLPC1-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
