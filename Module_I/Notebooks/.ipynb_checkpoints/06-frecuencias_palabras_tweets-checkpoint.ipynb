{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Construyendo y visualizando frecuencias de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado, nos centraremos en la función auxiliar `build_freqs()` y en la visualización de un conjunto de datos introducido en ella. En nuestro objetivo de análisis de sentimiento de tweets, esta función construirá un diccionario donde podemos buscar cuántas veces aparece una palabra en las listas de tweets positivos o negativos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos importando las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Plibrería para NLP\n",
    "from nltk.corpus import twitter_samples    # Ejemplo de conjunto de datos de Twitter de NLTK\n",
    "import matplotlib.pyplot as plt            # biblioteca para visualización\n",
    "import numpy as np                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importaremos algunas funciones auxiliares que proporcionamos en el archivo utils.py:\n",
    "\n",
    "* `process_tweet()`: Limpia el texto, lo convierte en token en palabras separadas, elimina las palabras vacías y convierte las palabras en raíces.\n",
    "* `build_freqs()`: Cuenta la frecuencia con la que una palabra del \"corpus\" (el conjunto completo de tweets) se asoció con una etiqueta positiva \"1\" o una etiqueta negativa \"0\". Luego construye el diccionario `freqs`, donde cada clave es una tupla` (palabra, etiqueta) `, y el valor es el recuento de su frecuencia dentro del corpus de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaddiel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import our convenience functions\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el conjunto de datos de muestra NLTK\n",
    "\n",
    "Utilizaremos la [base de datos de tweeter de NLTK](http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  10000\n"
     ]
    }
   ],
   "source": [
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, crearemos una matriz de etiquetas que coincida con los sentimientos de nuestros tweets. Este tipo de datos funciona de forma muy similar a una lista normal, pero está optimizado para cálculos y manipulación. La matriz `labels` estará compuesta por 10000 elementos. Los primeros 5000 se llenarán con etiquetas \"1\" que denotan sentimientos positivos, y los siguientes 5000 serán etiquetas \"0\" que denotan lo contrario. Podemos hacer esto fácilmente con una serie de operaciones proporcionadas por la biblioteca `numpy`:\n",
    "\n",
    "* `np.ones()` \n",
    "* `np.zeros()` \n",
    "* `np.append()` - concatena arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordatorio de uso de dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'key1': 1, 'key2': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key1': 0, 'key2': 2, 'key3': -5}\n"
     ]
    }
   ],
   "source": [
    "# Add a new entry\n",
    "dictionary['key3'] = -5\n",
    "\n",
    "# Overwrite the value of key1\n",
    "dictionary['key1'] = 0\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing values and lookup keys\n",
    "\n",
    "Performing dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this: \n",
    "\n",
    "* Using square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\n",
    "* Using the [get()](https://docs.python.org/3/library/stdtypes.html#dict.get) method: This allows us to set a default value if the dictionary key does not exist. \n",
    "\n",
    "Let us see these in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword `in`) before getting the item. On the other hand, you can use the `.get()` method if you want to set a default value when the key is not found. Let's compare these in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item found:  0\n",
      "item found:  0\n"
     ]
    }
   ],
   "source": [
    "# This prints a value\n",
    "if 'key1' in dictionary:\n",
    "    print(\"item found: \", dictionary['key1'])\n",
    "else:\n",
    "    print('key1 is not defined')\n",
    "\n",
    "# Same as what you get with get\n",
    "print(\"item found: \", dictionary.get('key1', -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key does not exist!\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# This prints a message because the key is not found\n",
    "if 'key7' in dictionary:\n",
    "    print(dictionary['key7'])\n",
    "else:\n",
    "    print('key does not exist!')\n",
    "\n",
    "# This prints -1 because the key is not found and we set the default to -1\n",
    "print(dictionary.get('key7', -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionario de frecuencia de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la función **build_freqs()** en **utils.py**. Esta es la función que crea el diccionario que contiene los recuentos de palabras de cada corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1    \n",
    "    return freqs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se muestra arriba, cada clave es una tupla de 2 elementos que contiene un par \"(palabra, y)\". La \"palabra\" es un elemento en un tweet procesado, mientras que \"y\" es un número entero que representa el corpus: \"1\" para los tweets positivos y \"0\" para los tweets negativos. El valor asociado con esta clave es el número de veces que esa palabra aparece en el corpus especificado. Por ejemplo:\n",
    "\n",
    "```\n",
    "# \"folowfriday\" aparece 25 veces en los tweets positivos\n",
    "('sigue el viernes', 1.0): 25\n",
    "\n",
    "# \"shame\" aparece 19 veces en los tweets negativos\n",
    "('shame', 0.0): 19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es el momento de usar el diccionario devuelto por la función `build_freqs()`. Primero, alimentemos nuestras listas de `tweets` y` etiquetas` y luego imprimamos un informe básico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the frequency of each word depending on its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafortunadamente, esto no ayuda mucho a comprender los datos. Sería mejor visualizar este resultado para obtener mejores conocimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de recuentos de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos un conjunto de palabras que nos gustaría visualizar. Es mejor almacenar esta información temporal en una tabla que sea muy fácil de usar más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "\n",
    "# list representing our table of word counts.\n",
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "data = []\n",
    "\n",
    "# loop through our selected words\n",
    "for word in keys:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos usar un diagrama de dispersión para inspeccionar esta tabla visualmente. En lugar de graficar los conteos sin procesar, lo trazaremos en la escala logarítmica para tener en cuenta las amplias discrepancias entre los conteos sin procesar (por ejemplo, `:)` tiene 3568 conteos en positivo mientras que solo 2 en negativo). La línea roja marca el límite entre las áreas positivas y negativas. Las palabras cercanas a la línea roja se pueden clasificar como neutrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "# do the same for the negative counts\n",
    "y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "# Plot a dot for each pair of words\n",
    "ax.scatter(x, y)  \n",
    "\n",
    "# assign axis labels\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "# Add the word as the label at the same position as you added the points just before\n",
    "for i in range(0, len(data)):\n",
    "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuadro es sencillo de interpretar. Muestra que los emoticones `:)` y `: (` son muy importantes para el análisis de sentimientos. Por lo tanto, ¡no debemos permitir que los pasos de preprocesamiento eliminen estos símbolos!\n",
    "\n",
    "¿Qué pasa con el símbolo de la corona?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
