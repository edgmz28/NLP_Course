{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Regresión Logística para Análisis de Sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "# this enables importing of these files without downloading it again when we refresh our workspace\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepara los datos\n",
    "* `twitter_samples` contiene subconjuntos de 5,000 tweets positivos, 5,000 tweets negativos y el conjunto completo de 10,000 tweets.\n",
    "     * Si utiliza los tres conjuntos de datos, introduciríamos duplicados de los tweets positivos y negativos.\n",
    "     * Seleccionará solo los cinco mil tweets positivos y los cinco mil tweets negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train test split: 20% para test, y 80% para train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creear una matriz de etiquetas positivas y negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11340\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "type(freqs) = <class 'dict'>\n",
    "len(freqs) = 11346\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento del tweet\n",
    "\n",
    "La función dada `process_tweet ()` tokeniza el tweet en palabras individuales, elimina las palabras vacías y aplica la derivación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "This is an example of a positive tweet: \n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    " \n",
    "This is an example of the processes version: \n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extrayendo las características\n",
    "\n",
    "* Dada una lista de tweets, extraiga las características y guárdelas en una matriz. Extraerás dos características.\n",
    "     * La primera característica es la cantidad de palabras positivas en un tweet.\n",
    "     * La segunda característica es la cantidad de palabras negativas en un tweet.\n",
    "* Luego entrene su clasificador de regresión logística en estas características.\n",
    "* Pruebe el clasificador en un conjunto de validación. \n",
    "\n",
    "### Implementación de la función de extract_features.\n",
    "* Esta función admite un solo tweet.\n",
    "* Procesaremos el tweet usando la función `process_tweet()` importada y la guardaremos en la lista de palabras del tweet.\n",
    "* Recorreremos cada palabra en la lista de palabras procesadas\n",
    "     * Para cada palabra, consultaremos el diccionario `freqs` para el recuento cuando esa palabra tiene una etiqueta positiva '1'. (con clave (palabra, 1.0)\n",
    "     * Hacemos lo mismo con el recuento para cuando la palabra esté asociada con la etiqueta negativa '0'. (con la clave (palabra, 0.0).)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Check your function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1.00e+00 3.02e+03 6.10e+01]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de la Regresión Logística\n",
    "\n",
    "\n",
    "### Función sigmoide\n",
    "Aprenderá a utilizar la regresión logística para la clasificación de texto.\n",
    "* La función sigmoidea se define como:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "Asigna la entrada 'z' a un valor que varía entre 0 y 1, por lo que puede tratarse como una probabilidad.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Testing your function \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')\n",
    "else:\n",
    "    print('Oops again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression y función sigmoide\n",
    "\n",
    "La regresión logística toma una regresión lineal regular y aplica un sigmoide a la salida de la regresión lineal.\n",
    "\n",
    "Regresion:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Tenga en cuenta que los valores $ \\theta $ son \"pesos\". Si realizó la especialización en aprendizaje profundo, nos referimos a los pesos con el vector `w`. En este curso, usamos una variable diferente $ \\theta $ para referirnos a los pesos.\n",
    "\n",
    "Regresión logística\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Nos referiremos a 'z' como los 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de costo y gradiente\n",
    "\n",
    "La función de costo utilizada para la regresión logística es el promedio de la pérdida de registro en todos los ejemplos de entrenamiento:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ es la cantidad de ejemplos de entrenamiento\n",
    "* $y^{(i)}$ es la etiqueta real del i-ésimo dato de entrenamiento.\n",
    "* $h(z(\\theta)^{(i)})$ es la predicción del modelo para el i-ésimo ejemplo de entrenamiento.\n",
    "\n",
    "La función de pérdida para un solo ejemplo de entrenamiento es\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* Todos los valores de $ h $ están entre 0 y 1, por lo que los registros serán negativos. Esa es la razón del factor -1 aplicado a la suma de los dos términos de pérdida.\n",
    "* Tenga en cuenta que cuando el modelo predice 1 ($ h (z (\\theta)) = 1 $) y la etiqueta $ y $ también es 1, la pérdida para ese ejemplo de entrenamiento es 0.\n",
    "* De manera similar, cuando el modelo predice 0 ($ h (z (\\theta)) = 0 $) y la etiqueta real también es 0, la pérdida para ese ejemplo de entrenamiento es 0.\n",
    "* Sin embargo, cuando la predicción del modelo es cercana a 1 ($ h (z (\\theta)) = 0.9999 $) y la etiqueta es 0, el segundo término de la pérdida logarítmica se convierte en un gran número negativo, que luego se multiplica por el factor general de -1 para convertirlo en un valor de pérdida positivo. $ -1 \\times (1 - 0) \\times log (1 - 0.9999) \\approx 9.2 $ Cuanto más se acerque la predicción del modelo a 1, mayor será la pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Del mismo modo, si el modelo predice cerca de 0 ($ h (z) = 0.0001 $) pero la etiqueta real es 1, el primer término en la función de pérdida se convierte en un número grande: $ -1 \\times log (0.0001) \\approx 9.2 $. Cuanto más cercana sea la predicción a cero, mayor será la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizar los pesos\n",
    "\n",
    "Para actualizar su vector de peso $ \\theta $, aplicará el descenso de gradiente para mejorar iterativamente las predicciones de su modelo.\n",
    "El gradiente de la función de costo $ J $ con respecto a uno de los pesos $ \\theta_j $ es:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' es el índice de todos los ejemplos de formación \"m\".\n",
    "* 'j' es el índice del peso $ \\theta_j $, entonces $ x_j $ es la característica asociada con el peso $ \\theta_j $\n",
    "\n",
    "* Para actualizar el peso $ \\theta_j $, lo ajustamos restando una fracción del gradiente determinado por $ \\alpha $:\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j} J (\\theta) $$\n",
    "* La tasa de aprendizaje $ \\alpha $ es un valor que elegimos para controlar qué tan grande será una sola actualización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de la función Gradiente Descendente\n",
    "* El número de iteraciones `num_iters` es el número de veces que utilizará todo el conjunto de entrenamiento.\n",
    "* Para cada iteración, calculará la función de costo usando todos los ejemplos de entrenamiento (hay ejemplos de entrenamiento `m`), y para todas las funciones.\n",
    "* En lugar de actualizar un solo peso $ \\theta_i $ a la vez, podemos actualizar todos los pesos en el vector de columna:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $ \\mathbf {\\theta} $ tiene dimensiones (n + 1, 1), donde 'n' es el número de características, y hay un elemento más para el término de sesgo $ \\theta_0 $ (tenga en cuenta que el valor de característica correspondiente $ \\mathbf {x_0} $ es 1).\n",
    "* Los 'logits', 'z', se calculan multiplicando la matriz de características 'x' con el vector de peso 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* La predicción 'h' se calcula aplicando el sigmoide a cada elemento en 'z': $ h (z) = sigmoid (z) $, y tiene dimensiones (m, 1).\n",
    "* La función de costo $ J $ se calcula tomando el producto escalar de los vectores 'y' y 'log (h)'. Dado que tanto 'y' como 'h' son vectores de columna (m, 1), transponga el vector a la izquierda, de modo que la multiplicación de matrices de un vector de fila con un vector de columna realice el producto escalar.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* La actualización de theta también está vectorizada. Debido a que las dimensiones de $ \\mathbf {x} $ son (m, n + 1), y tanto $ \\mathbf {h} $ como $ \\mathbf {y} $ son (m, 1), necesitamos transponer $ \\mathbf {x} $ y colóquelo a la izquierda para realizar la multiplicación de matrices, que luego da la respuesta (n + 1, 1) que necesitamos:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 3: Modelo de Regresión Logística para Análisis de Sentimientos de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Entrenar el modelo\n",
    "\n",
    "**instrucciones: Para entrenar el modelo:**\n",
    "\n",
    "* Apile las características de todos los datos de entrenamiento en una matriz `X`.\n",
    "* Llame la función `gradientDescent`, que se implementó anteriormente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24216477.\n",
      "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "The cost after training is 0.24216529.\n",
    "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Probando la regresión logística\n",
    "\n",
    "**Instrucciones para Predecir si un tweet es positivo o negativo**\n",
    "\n",
    "* Dado un tweet, procéselo y luego extraiga las características.\n",
    "* Aplicar los pesos aprendidos del modelo para obtener los logits.\n",
    "* Aplicar la función sigmoide a los logits para obtener la predicción (un valor entre 0 y 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.518580\n",
      "I am bad -> 0.494339\n",
      "this movie should have been great. -> 0.515331\n",
      "great -> 0.515464\n",
      "great great -> 0.530898\n",
      "great great great -> 0.546273\n",
      "great great great great -> 0.561561\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```\n",
    "I am happy -> 0.518580\n",
    "I am bad -> 0.494339\n",
    "this movie should have been great. -> 0.515331\n",
    "great -> 0.515464\n",
    "great great -> 0.530898\n",
    "great great great -> 0.546273\n",
    "great great great great -> 0.561561\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81636482]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Verificando el rendimiento del modelo\n",
    "\n",
    "Después de entrenar su modelo con el conjunto de entrenamiento anterior, verifique cómo podría funcionar su modelo en datos reales no vistos probándolo con el conjunto de prueba.\n",
    "\n",
    "**Instrucciones: Implementar `test_logistic_regression`**\n",
    "\n",
    "* Dados los datos de prueba y los pesos de su modelo entrenado, calcule la precisión de su modelo de regresión logística.\n",
    "* Utilice su función `predict_tweet ()` para hacer predicciones en cada tweet en el conjunto de prueba.\n",
    "* Si la predicción es> 0,5, establezca la clasificación del modelo `y_hat` en 1; de lo contrario, establezca la clasificación del modelo` y_hat` en 0.\n",
    "* Una predicción es precisa cuando `y_hat` es igual a` test_y`. Para calcular la precisón sume todas las instancias en las que ($y_hat==test_y$) sean iguales y divida por `m`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output: \n",
    "```0.9950```  \n",
    "Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.- Predice tu propio tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "[[0.48139091]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
